{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param w: weights\n",
    "    :return grad: gradient\n",
    "    \"\"\"\n",
    "    grad = (-1/len(y))*tx.T@(y-tx@w)\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Least square gradient descent\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: max number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :return ws: weights\n",
    "    :return ls: loss\n",
    "    \"\"\"    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w-gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param w: weights\n",
    "    :return grad: gradient\n",
    "    \"\"\"    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    grad = -1/len(y)*tx.T@e\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Least square stochastic gradient descent\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param initial_w: initial weights\n",
    "    :param batch_size: 1 if sgd\n",
    "    :param max_iter: max number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :return ws: weights\n",
    "    :return ls: loss\n",
    "    \"\"\"   \n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    \"\"\"\n",
    "    Solves the closed form least square equation to obtain optimal weights\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :returns w,l: weights and loss of the model\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    l = compute_loss2(y, tx, w)\n",
    "    return w, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Solves the closed form of Ridge regression equation to obtain optimal weights\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param lambda_: regulizer\n",
    "    :returns w,l: weights and loss of the model\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T@tx+lambda_*np.eye(tx.shape[1]),tx.T@y)\n",
    "    l = compute_loss(y, tx, w)\n",
    "    return w, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \n",
    "    :param z: \n",
    "    :return z:\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Update weights function for logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def LR_loss_function(y, tx, w):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    \n",
    "    :return loss: logistic loss\n",
    "    \"\"\" \n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    loss = (error1-error2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(y, tx, initial_w, gamma)\n",
    "        loss = LR_loss_function(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_LR_update_weights(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Update weights function for  regularized logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y) + lambda_ * w\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def reg_LR_loss_function(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: logistic loss\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    # the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    # the error when label=0\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    # return average of sum of costs\n",
    "    return (error1-error2).mean()+lambda_/2*np.dot(w.T,w)/ len(tx)\n",
    "\n",
    "\n",
    "# regularized logistic regression function\n",
    "def reg_logistic_regression(y,tx, initial_w,max_iter, gamma,lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        if(iter_n > 200):\n",
    "            gamma = gamma-gamma/40\n",
    "        w = reg_LR_update_weights(y, tx, initial_w, gamma,lambda_)\n",
    "        loss = reg_LR_loss_function(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    compute the accuracy\n",
    "    \n",
    "    :param y_pred: predictions\n",
    "    :param y: real labels\n",
    "    \n",
    "    :return acc: accuracy\n",
    "    \"\"\"\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    acc = np.count_nonzero(arr==0) / len(y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    build k indices for k-fold.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param k_fold: number of folds\n",
    "    :param seed: seed for randomization\n",
    "    \n",
    "    :return k_indices: indices \n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    :param x: matrix \n",
    "    :param degree: degree of expansion\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices,k, regression_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes cross validation on a given data set using a given regression method, and computes the\n",
    "    weights, the train loss, the test loss, and the train and loss accuracy\n",
    "    if the degree is not none, it will perform feature expansion on the data set\n",
    "    \n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_indices: k_fold already randomly computed indices\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param logistic: boolean; if true, the loss used is the logistic one\n",
    "    :param **kwargs: differents parameters such as the regulizer lambda or the learning rate gamma\n",
    "    \"\"\"\n",
    "    degree = kwargs[\"degree\"]\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "    \n",
    "\n",
    "    w_initial = np.zeros(x_train.shape[1])\n",
    "    kwargs = kwargs\n",
    "    kwargs['initial_w'] = w_initial\n",
    "\n",
    "    aug = False\n",
    "    if regression_method is reg_logistic_regression:\n",
    "        w, loss_train = regression_method(y_train, x_train, kwargs['initial_w'],  kwargs['max_iter'], kwargs['gamma'], kwargs['lambda_'])\n",
    "        loss_test = reg_LR_loss_function(y_test, x_test, w ,kwargs['lambda_'])\n",
    "        \n",
    "    elif regression_method is logistic_regression:\n",
    "        w, loss_train = regression_method(y_train, x_train, kwargs['initial_w'],  kwargs['max_iter'], kwargs['gamma'])\n",
    "        loss_test = LR_loss_function(y_test, x_test, w)\n",
    "        \n",
    "    elif regression_method is least_square:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "    elif regression_method is ridge_regression:\n",
    "        w, loss_train = regression_method(y_train,x_train, kwargs['lambda_'])\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    elif regression_method is least_squares_SGD: \n",
    "        aug = True\n",
    "        y_test = (y_test*2)-1\n",
    "        y_train = (y_train*2)-1\n",
    "        w, loss_train = regression_method(y_train, x_train, kwargs['initial_w'],kwargs['batch_size'],  kwargs['max_iter'], kwargs['gamma'])\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    else:\n",
    "        aug = True\n",
    "        y_test = (y_test*2)-1\n",
    "        y_train = (y_train*2)-1\n",
    "        w, loss_train = regression_method(y_train, x_train, kwargs['initial_w'],  kwargs['max_iter'], kwargs['gamma'])\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "\n",
    "    if not aug:\n",
    "        y_test = (y_test*2)-1\n",
    "        y_train = (y_train*2)-1\n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n",
    "\n",
    "def evaluate(tx, wf, degree):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to evaluate weights over all the train model\n",
    "    \n",
    "    :param tx: train features\n",
    "    :param wf: wights to evaluate\n",
    "    :param degree: degree of expansion\n",
    "    :return acc: accuracy of the weights over the train model\n",
    "    \"\"\"\n",
    "    if degree is not None:\n",
    "        tx =build_poly(tx, degree)\n",
    "    if isinstance(wf, list):\n",
    "        wk =np.mean(wf, axis =0)\n",
    "\n",
    "    else:\n",
    "        wk = wf\n",
    "                \n",
    "    y_pred = predict_labels(wk, tx)\n",
    "    acc = compute_accuracy(y_std*2-1, y_pred)\n",
    "    return acc\n",
    "def remove_outliers_IQR(tx, y_, high,low):\n",
    "    \"\"\"\n",
    "    removes outliers using IQR\n",
    "    \n",
    "    :param tx: features\n",
    "    :param y: labels\n",
    "    :param high: high IQR\n",
    "    :param low: low IQR\n",
    "    :returns tX, Y: features and labels without outliers\n",
    "    \"\"\"\n",
    "    Q1 = np.quantile(tx,low,axis = 0)\n",
    "    Q3 = np.quantile(tx,high, axis = 0)\n",
    "    IQR = Q3 - Q1\n",
    "    tX_no_outliers = tx[~((tx < (Q1 - 1.5 * IQR)) |(tx > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    y_no_outliers = y_[~((tx < (Q1 - 1.5 * IQR)) |(tx > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return tX_no_outliers, y_no_outliers\n",
    "\n",
    "def replace(tx, value):\n",
    "    \"\"\"\n",
    "    Replaces invalid values with the mean of all the values in the cooresponding feature \n",
    "\n",
    "    :param tX: features\n",
    "    :param value: value to replace\n",
    "    :return tX: tX with replaced values\n",
    "    \"\"\"\n",
    "    for i in range(tx.shape[1]):\n",
    "        data = tx[:, i].copy()\n",
    "        np.delete(data, np.where(data == value)) \n",
    "        if(np.median(data) == -999):\n",
    "            data_median = 0  \n",
    "        else :\n",
    "            data_median = np.median(data)\n",
    "        tx[tx[:, i] == value,i] = data_median  \n",
    "    return tx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,y,tx,seed=0, **kwargs):\n",
    "    \"\"\"\n",
    "    regularized logistic regression function \n",
    "    \n",
    "    :param Model: model that we'll use\n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_fold: number of folds\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param seed: random seed for cross validation split\n",
    "    :param **kwargs: multiple possible parameters\n",
    "    \n",
    "    :return wf: final weights \n",
    "    \"\"\"    \n",
    "    weights = []\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    k_fold = kwargs[\"k_fold\"]\n",
    "    \n",
    "    if kwargs[\"degree\"] == 0:\n",
    "        kwargs[\"degree\"] = None\n",
    "    degree = kwargs[\"degree\"]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in tqdm(range(k_fold)):\n",
    "        w, loss_train, loss_test, accuracy_train, accuracy_test = cross_validation(y, tx, k_indices,k, model, **kwargs)\n",
    "        weights.append(w)\n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_test)\n",
    "        accuracies_train.append(accuracy_train)\n",
    "        accuracies_test.append(accuracy_test)\n",
    "    leg = [\"train loss \"+str(i) for i in range(k_fold)]\n",
    "    plt.legend(leg)\n",
    "    plt.plot(losses_train[-1])    \n",
    "    print(\"<-\"+\"-\"*75+\"->\")\n",
    "    if degree is not None:\n",
    "        degree = int(degree)\n",
    "    print(\"{:15.14}|{:15.14}|{:15.14}|{:15.14}|{:15.14}\".format(\"Train losses\",\"Test losses\",\"Train accuracy\",\"Test Accuracy\", \"Evaluation\"))\n",
    "    for i in range(k_fold):\n",
    "        if model is least_square or model is ridge_regression:\n",
    "            print(\"{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}\".format(losses_train[i], losses_test[i] ,accuracies_train[i], accuracies_test[i], evaluate(tX_std, np.array(weights[i]), degree)))\n",
    "        else:\n",
    "            print(\"{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}\".format(losses_train[i][-1], losses_test[i] ,accuracies_train[i], accuracies_test[i], evaluate(tX_std, np.array(weights[i]), degree)))\n",
    "        print(\"{:15.1}|{:15.14}|{:15.14}|{:15.14}|{:15.14}\".format(\"\",\"\",\"\",\"\",\"\"))\n",
    "    print(\"<-\"+\"-\"*75+\"->\")\n",
    "    ev = evaluate(tX_std, weights, degree)\n",
    "    print(f\"evaluation mean w: {ev}\")\n",
    "\n",
    "    return ev\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y_std = (y+1)/2\n",
    "tX = replace(tX, -999)\n",
    "tX_std, tX_mean, tX_stdev  = standardize(tX)\n",
    "\n",
    "tX_no_outliers, y_no_outliers = remove_outliers_IQR(tX,y_std, 0.87, 0)\n",
    "\n",
    "tX_no_outliers_std, _, _ = standardize(tX_no_outliers,tX_mean,tX_stdev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_no_outliers_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "print(parameters[\"LS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters[\"LR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls = train(locals()[parameters[\"RR\"][\"f_name\"]],y_std,tX_std,seed=0,**parameters[\"RR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"LS_GD\"][\"degree\"]=2\n",
    "print(parameters[\"LS_GD\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least squares Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lsGD = train(least_squares_GD,y_no_outliers,tX_no_outliers_std,seed=0,**parameters[\"LS_GD\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least squares Stochastic Gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lsGD = train(least_squares_GD,y_no_outliers,tX_no_outliers_std,seed=0,**parameters[\"LS_SGD\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters[\"LR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_LR = train(logistic_regression,y_no_outliers,tX_no_outliers_std,seed = 20,**parameters['LR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gamma LR grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "parameters[\"LR\"][\"k_fold\"] = 3\n",
    "gam_ = np.linspace(20,65,100) / 100\n",
    "parameters[\"LR\"][\"max_iter\"] = 300\n",
    "for i in tqdm(range(len(gam_))):\n",
    "    parameters[\"LR\"][\"gamma\"] = gam_[i]\n",
    "    e = train(logistic_regression,y_no_outliers,tX_no_outliers_std,seed = 0,**parameters['LS_GD'])\n",
    "    result.append(e)\n",
    "    \n",
    "result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('plotvalues.csv', result, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gam_,result)\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"LR\"][\"k_fold\"] = 5\n",
    "parameters[\"LR\"][\"gamma\"] = 0.6136363636363636\n",
    "\n",
    "parameters[\"LR\"][\"max_iter\"] = 300\n",
    "e = train(logistic_regression,y_no_outliers,tX_no_outliers_std,seed = 0,**parameters['LR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"LR\"][\"k_fold\"]  =8\n",
    "\n",
    "w_LR = train(logistic_regression,y_no_outliers,tX_no_outliers_std,seed = 0,**parameters['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters[\"LR\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test regularized Logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[\"R_LR\"][\"gamma\"]  =0.53\n",
    "\n",
    "w_regLR = train(reg_logistic_regression,y_no_outliers,tX_no_outliers_std,**parameters['R_LR'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = [least_squares_GD,least_square, ridge_regression, logistic_regression,reg_logistic_regression]\n",
    "initial_w  = np.zeros(tX_std.shape[1])\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "gammas = np.array([0.0001,0.001,0.01,0.1])\n",
    "degrees = np.array([0,1,2,3,4,5,6,7])\n",
    "lambda_ = lambdas[0]\n",
    "gamma = gammas[0]\n",
    "degree = degrees[0]\n",
    "bestParametersAccuracyTrain = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "bestParametersAccuracyTest = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "bestParametersAll= [[names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ],\n",
    "                   [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ],\n",
    "                   [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ],\n",
    "                   [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ],\n",
    "                   [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]]\n",
    "for indexML, model_name in enumerate(names):\n",
    "        print(\"Done about :\" + str(indexML/len(names)))\n",
    "        if(model_name is least_square or model_name is ridge_regression): #Skip the lambdas\n",
    "            if(model_name is ridge_regression):\n",
    "                for indexL,lambda_ in enumerate(lambdas):\n",
    "                    parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                    wTemp, accTr, accTe = train(names[indexML],y_no_outliers,tX_no_outliers_std,5,None,0,**parameters[indexML])                \n",
    "                    if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                        bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                    if accTe > bestParametersAccuracyTest[4]:\n",
    "                        bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "                    if accTe > bestParametersAll[indexML][4]:\n",
    "                        bestParametersAll[indexML] = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "            else:\n",
    "                parameteres = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                wTemp, accTr, accTe = train(names[indexML],y_no_outliers,tX_no_outliers_std,5,None,0,**parameters[indexML])                \n",
    "                if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                    bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                if accTe > bestParametersAccuracyTest[4]:\n",
    "                    bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp] \n",
    "                if accTe > bestParametersAll[indexML][4]:\n",
    "                    bestParametersAll[indexML] = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "        else:    \n",
    "            for indexG,gamma in enumerate(gammas):\n",
    "                if(model_name is reg_logistic_regression):\n",
    "                    for indexL,lambda_ in enumerate(lambdas):\n",
    "                        parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                        wTemp, accTr, accTe = train(names[indexML],y_no_outliers,tX_no_outliers_std,5,None,0,**parameters[indexML])\n",
    "                        if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                            bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                        if accTe > bestParametersAccuracyTest[4]:\n",
    "                            bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "                        if accTe > bestParametersAll[indexML][4]:\n",
    "                            bestParametersAll[indexML] = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "                else:\n",
    "                    parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                    wTemp, accTr, accTe = train(names[indexML],y_no_outliers,tX_no_outliers_std,5,None,0,**parameters[indexML])\n",
    "                    if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                        bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                    if accTe > bestParametersAccuracyTest[4]:\n",
    "                        bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp] \n",
    "                    if accTe > bestParametersAll[indexML][4]:\n",
    "                        bestParametersAll[indexML] = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "               \n",
    "                           \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bestParametersAccuracyTrain)\n",
    "print(bestParametersAccuracyTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test_std,_,_ = standardize(tX_test, tX_mean, tX_stdev)\n",
    "tX_test_std =build_poly(tX_test_std, 2)\n",
    "OUTPUT_PATH = 'outLogRegD23v2.csv' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(np.mean(w_LR[0],axis=0), tX_test_std[0])\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tX_test_std[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
