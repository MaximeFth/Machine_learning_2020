{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from costs import compute_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    return (-1/len(y))*tx.T@(y-tx@w)\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"least square gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w-gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    return -1/len(y)*tx.T@e\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Least square stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # Define parameters to store w and loss\n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        \n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        print(grad)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        #print(\"stoch Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return np.array(losses), np.array(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls, loss = least_square(y_std, tX_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamda):\n",
    "    w = np.linalg.solve(tx.T@tx+lamda*np.eye(tx.shape[1]),tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7779046298744507\n"
     ]
    }
   ],
   "source": [
    "wls, loss = ridge_regression(y, tX_std,0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(tx, y, w, gamma):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def loss_function_LR(tx, y, w):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    return (error1-error2).mean()\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(tx, y, initial_w, gamma)\n",
    "        loss = loss_function_LR(tx, y, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    return np.array(losses)[-1], np.array(ws)[-1]\n",
    "\n",
    "#################################################################################\n",
    "def decision_boundary(prob):\n",
    "    return 1 if prob > 0.5 else -1\n",
    "\n",
    "def classify(predictions):\n",
    "    decision_boundary = np.vectorize(decision_boundary)\n",
    "    return decision_boundary(predictions).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_LR_update_weights(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Update weights function for  regularized logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y) + lambda_ * w\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def reg_LR_loss_function(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: logistic loss\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    # the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    # the error when label=0\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    # return average of sum of costs\n",
    "    return (error1-error2).mean()+lambda_/2*np.dot(w.T,w)/ len(tx)\n",
    "\n",
    "\n",
    "# regularized logistic regression function\n",
    "def reg_logistic_regression(y,tx, initial_w,max_iter, gamma,lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = reg_LR_update_weights(y, tx, initial_w, gamma,lambda_)\n",
    "        loss = reg_LR_loss_function(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    compute the accuracy\n",
    "    \n",
    "    :param y_pred: predictions\n",
    "    :param y: real labels\n",
    "    \n",
    "    :return acc: accuracy\n",
    "    \"\"\"\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    acc = np.count_nonzero(arr==0) / len(y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    build k indices for k-fold.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param k_fold: number of folds\n",
    "    :param seed: seed for randomization\n",
    "    \n",
    "    :return k_indices: indices \n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, logistic, regression_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes cross validation on a given data set using a given regression method, and computes the\n",
    "    weights, the train loss, the test loss, and the train and loss accuracy\n",
    "    if the degree is not none, it will perform feature expansion on the data set\n",
    "    \n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_indices: k_fold already randomly computed indices\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param logistic: boolean; if true, the loss used is the logistic one\n",
    "    :param **kwargs: differents parameters such as the regulizer lambda or the learning rate gamma\n",
    "    \"\"\"\n",
    "    k = k-1\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "\n",
    "    \n",
    "    w_initial = np.zeros(x_train.shape[1])\n",
    "    kwargs = kwargs\n",
    "    kwargs['initial_w'] = w_initial\n",
    "\n",
    "    w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "    if logistic == True:\n",
    "        loss_test = reg_LR_loss_function(y_test, x_test, w ,kwargs['lambda_'])\n",
    "    else:\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "    y_test = (y_test*2)-1\n",
    "    y_train = (y_train*2)-1\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)\n",
    "y_std = (y+1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,y,tx,k_fold,degree,seed=0, **kwargs):\n",
    "    \"\"\"\n",
    "    regularized logistic regression function \n",
    "    \n",
    "    :param Model: model that we'll use\n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_fold: number of folds\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param seed: random seed for cross validation split\n",
    "    :param **kwargs: multiple possible parameters\n",
    "    \n",
    "    :return wf: final weights \n",
    "    \"\"\"    \n",
    "    logistic = False\n",
    "    if model is logistic_regression or model is reg_logistic_regression:\n",
    "        logistic = True\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    w, loss_train, loss_test, accuracy_train, accuracy_test = cross_validation(y, tx, k_indices, k_fold, degree, logistic, model,max_iter=200, **kwargs)\n",
    "    plt.plot(loss_train)\n",
    "    print(f'loss_train: {loss_train[-1]}, loss_test: {loss_test}, accuracy_train: {accuracy_train}, accuracy_test: {accuracy_test}')\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train: 0.312494806125696, loss_test: 0.30816617018122555, accuracy_train: 0.672625, accuracy_test: 0.66918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wf = train(least_squares_GD,y_std,tX_std,5,None,0,gamma=0.0017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-914badaba4d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtX_test_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'outLogRegD23v2.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wk' is not defined"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# standardize wrt the tx train mean and std\n",
    "def standardize_test(tx, tx_test):\n",
    "    centered_data = tx_test - np.mean(tx, axis=0)\n",
    "    std_data = centered_data / np.std(tx, axis=0)\n",
    "    return std_data\n",
    "tX_test_std = standardize_test(tX, tX_test)\n",
    "OUTPUT_PATH = 'outLogRegD23v2.csv' \n",
    "y_pred = predict_labels(wk, tX_test_std)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk = np.mean(wf,axis=0)\n",
    "ff = predict_labels(wk[:30], tX_std)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
