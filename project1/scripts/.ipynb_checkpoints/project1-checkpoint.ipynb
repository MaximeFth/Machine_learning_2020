{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import costs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "tX = standardize(tX)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import *\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    return gradient_descent(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    return stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): loss=1.0, w0=0.006061825655066102, w1=-0.009503412406816736\n",
      "Gradient Descent(1/199): loss=0.8802840084893263, w0=0.014317624669673347, w1=-0.01054167087366943\n",
      "Gradient Descent(2/199): loss=0.8735573967179924, w0=0.021868803122944837, w1=-0.012827418185016225\n",
      "Gradient Descent(3/199): loss=0.869936791164917, w0=0.029184583764175705, w1=-0.014844014450596544\n",
      "Gradient Descent(4/199): loss=0.8666846988754907, w0=0.036205585941766885, w1=-0.016834066393043107\n",
      "Gradient Descent(5/199): loss=0.8637003309064927, w0=0.04295448068865377, w1=-0.01876418698173058\n",
      "Gradient Descent(6/199): loss=0.860952504439469, w0=0.04944038847035968, w1=-0.020643918588195555\n",
      "Gradient Descent(7/199): loss=0.8584157790384571, w0=0.05567392957853871, w1=-0.022475772070868552\n",
      "Gradient Descent(8/199): loss=0.8560682132801115, w0=0.06166492381386983, w1=-0.02426307038098366\n",
      "Gradient Descent(9/199): loss=0.8538907600563397, w0=0.06742279933765516, w1=-0.026008735633455873\n",
      "Gradient Descent(10/199): loss=0.8518668029175537, w0=0.07295656925951441, w1=-0.027715495858931218\n",
      "Gradient Descent(11/199): loss=0.8499817769651655, w0=0.07827486970938251, w1=-0.02938586939732066\n",
      "Gradient Descent(12/199): loss=0.8482228572187749, w0=0.08338598323162343, w1=-0.03102218389407435\n",
      "Gradient Descent(13/199): loss=0.8465787015219409, w0=0.08829786040474043, w1=-0.03262658939161276\n",
      "Gradient Descent(14/199): loss=0.8450392376555796, w0=0.09301813844298165, w1=-0.03420107169526647\n",
      "Gradient Descent(15/199): loss=0.8435954863757952, w0=0.09755415759903172, w1=-0.03574746493523721\n",
      "Gradient Descent(16/199): loss=0.8422394137199907, w0=0.10191297573978797, w1=-0.03726746342761251\n",
      "Gradient Descent(17/199): loss=0.8409638072216447, w0=0.10610138143756859, w1=-0.03876263278584695\n",
      "Gradient Descent(18/199): loss=0.8397621717097273, w0=0.11012590584358051, w1=-0.040234420282937486\n",
      "Gradient Descent(19/199): loss=0.838628641197585, w0=0.11399283355793745, w1=-0.04168416447527856\n",
      "Gradient Descent(20/199): loss=0.8375579040307614, w0=0.11770821266711591, w1=-0.04311310411070292\n",
      "Gradient Descent(21/199): loss=0.8365451389970487, w0=0.12127786408517652, w1=-0.04452238635057142\n",
      "Gradient Descent(22/199): loss=0.8355859605314844, w0=0.12470739030740466, w1=-0.04591307434066222\n",
      "Gradient Descent(23/199): loss=0.8346763714949715, w0=0.12800218366293178, w1=-0.04728615416849055\n",
      "Gradient Descent(24/199): loss=0.8338127222843101, w0=0.13116743413528462, w1=-0.048642541246073784\n",
      "Gradient Descent(25/199): loss=0.8329916752568871, w0=0.13420813680579832, w1=-0.04998308615741673\n",
      "Gradient Descent(26/199): loss=0.8322101736357246, w0=0.13712909896370093, w1=-0.0513085800094314\n",
      "Gradient Descent(27/199): loss=0.8314654142083798, w0=0.1399349469178626, w1=-0.05261975932385679\n",
      "Gradient Descent(28/199): loss=0.8307548232531355, w0=0.14263013253823983, w1=-0.05391731050618802\n",
      "Gradient Descent(29/199): loss=0.8300760352234254, w0=0.14521893954955442, w1=-0.055201873925803314\n",
      "Gradient Descent(30/199): loss=0.8294268738008188, w0=0.14770548959543814, w1=-0.05647404763949665\n",
      "Gradient Descent(31/199): loss=0.828805334991658, w0=0.15009374808789705, w1=-0.05773439078856573\n",
      "Gradient Descent(32/199): loss=0.828209571995408, w0=0.15238752985431742, w1=-0.05898342669753023\n",
      "Gradient Descent(33/199): loss=0.8276378816161543, w0=0.154590504592189, w1=-0.06022164570050786\n",
      "Gradient Descent(34/199): loss=0.8270886920243381, w0=0.15670620214013853, w1=-0.061449507719287866\n",
      "Gradient Descent(35/199): loss=0.8265605517051846, w0=0.15873801757264766, w1=-0.06266744461523524\n",
      "Gradient Descent(36/199): loss=0.8260521194545234, w0=0.16068921612489487, w1=-0.06387586233534899\n",
      "Gradient Descent(37/199): loss=0.8255621553028191, w0=0.16256293795344817, w1=-0.06507514287109221\n",
      "Gradient Descent(38/199): loss=0.8250895122649293, w0=0.16436220273799415, w1=-0.06626564604701561\n",
      "Gradient Descent(39/199): loss=0.8246331288270862, w0=0.16608991412887933, w1=-0.06744771115470997\n",
      "Gradient Descent(40/199): loss=0.8241920220942647, w0=0.1677488640449319, w1=-0.06862165844624582\n",
      "Gradient Descent(41/199): loss=0.8237652815309511, w0=0.16934173682579817, w1=-0.06978779049998723\n",
      "Gradient Descent(42/199): loss=0.8233520632366406, w0=0.17087111324285312, w1=-0.07094639347049642\n",
      "Gradient Descent(43/199): loss=0.8229515847044347, w0=0.17233947437260966, w1=-0.07209773823317167\n",
      "Gradient Descent(44/199): loss=0.8225631200171414, w0=0.17374920533644797, w1=-0.07324208143327904\n",
      "Gradient Descent(45/199): loss=0.8221859954404156, w0=0.17510259891040322, w1=-0.07437966644813909\n",
      "Gradient Descent(46/199): loss=0.8218195853769348, w0=0.17640185900868163, w1=-0.07551072427041178\n",
      "Gradient Descent(47/199): loss=0.8214633086494162, w0=0.17764910404451628, w1=-0.07663547431967636\n",
      "Gradient Descent(48/199): loss=0.8211166250836371, w0=0.17884637017192032, w1=-0.07775412518882571\n",
      "Gradient Descent(49/199): loss=0.8207790323655163, w0=0.1799956144118441, w1=-0.07886687533117795\n",
      "Gradient Descent(50/199): loss=0.8204500631488627, w0=0.18109871766619354, w1=-0.07997391369365012\n",
      "Gradient Descent(51/199): loss=0.8201292823926611, w0=0.182157487623115, w1=-0.08107542030083206\n",
      "Gradient Descent(52/199): loss=0.8198162849087312, w0=0.18317366155690065, w1=-0.08217156679433962\n",
      "Gradient Descent(53/199): loss=0.8195106931023701, w0=0.18414890902581282, w1=-0.08326251693141146\n",
      "Gradient Descent(54/199): loss=0.819212154890163, w0=0.18508483447106963, w1=-0.08434842704633787\n",
      "Gradient Descent(55/199): loss=0.8189203417805424, w0=0.18598297972017405, w1=-0.0854294464779704\n",
      "Gradient Descent(56/199): loss=0.8186349471039471, w0=0.1868448263977072, w1=-0.08650571796625385\n",
      "Gradient Descent(57/199): loss=0.8183556843805588, w0=0.18767179824664199, w1=-0.08757737802044496\n",
      "Gradient Descent(58/199): loss=0.8180822858146208, w0=0.18846526336316777, w1=-0.08864455726143163\n",
      "Gradient Descent(59/199): loss=0.8178145009052724, w0=0.1892265363479475, w1=-0.08970738074033964\n",
      "Gradient Descent(60/199): loss=0.8175520951646624, w0=0.18995688037666067, w1=-0.09076596823540987\n",
      "Gradient Descent(61/199): loss=0.8172948489348776, w0=0.19065750919261404, w1=-0.09182043452894431\n",
      "Gradient Descent(62/199): loss=0.8170425562959076, w0=0.19132958902413058, w1=-0.09287088966595239\n",
      "Gradient Descent(63/199): loss=0.8167950240575069, w0=0.19197424042935532, w1=-0.09391743919597881\n",
      "Gradient Descent(64/199): loss=0.8165520708283825, w0=0.19259254007104412, w1=-0.09496018439945819\n",
      "Gradient Descent(65/199): loss=0.8163135261566726, w0=0.19318552242382928, w1=-0.09599922249981879\n",
      "Gradient Descent(66/199): loss=0.8160792297361609, w0=0.19375418141638293, w1=-0.0970346468624473\n",
      "Gradient Descent(67/199): loss=0.8158490306731074, w0=0.19429947201082837, w1=-0.09806654718152615\n",
      "Gradient Descent(68/199): loss=0.8156227868089934, w0=0.19482231172167708, w1=-0.09909500965566437\n",
      "Gradient Descent(69/199): loss=0.8154003640948377, w0=0.19532358207649955, w1=-0.10012011715316167\n",
      "Gradient Descent(70/199): loss=0.8151816360130962, w0=0.19580413002046798, w1=-0.10114194936767087\n",
      "Gradient Descent(71/199): loss=0.8149664830434548, w0=0.19626476926684033, w1=-0.1021605829649576\n",
      "Gradient Descent(72/199): loss=0.8147547921691292, w0=0.19670628159538844, w1=-0.10317609172139543\n",
      "Gradient Descent(73/199): loss=0.8145464564205427, w0=0.19712941810070583, w1=-0.10418854665477982\n",
      "Gradient Descent(74/199): loss=0.8143413744534945, w0=0.19753490039226707, w1=-0.10519801614799516\n",
      "Gradient Descent(75/199): loss=0.8141394501591652, w0=0.19792342174804634, w1=-0.10620456606602365\n",
      "Gradient Descent(76/199): loss=0.8139405923035024, w0=0.19829564822344165, w1=-0.10720825986674487\n",
      "Gradient Descent(77/199): loss=0.8137447141937304, w0=0.19865221971719024, w1=-0.10820915870593754\n",
      "Gradient Descent(78/199): loss=0.8135517333698913, w0=0.19899375099590225, w1=-0.10920732153686157\n",
      "Gradient Descent(79/199): loss=0.8133615713195009, w0=0.19932083267878203, w1=-0.11020280520476843\n",
      "Gradient Descent(80/199): loss=0.8131741532135389, w0=0.1996340321840514, w1=-0.1111956645366602\n",
      "Gradient Descent(81/199): loss=0.8129894076621375, w0=0.199933894638534, w1=-0.11218595242659225\n",
      "Gradient Descent(82/199): loss=0.8128072664884578, w0=0.2002209437518086, w1=-0.11317371991679219\n",
      "Gradient Descent(83/199): loss=0.8126276645193612, w0=0.20049568265628714, w1=-0.11415901627484693\n",
      "Gradient Descent(84/199): loss=0.8124505393915872, w0=0.2007585947145244, w1=-0.11514188906719044\n",
      "Gradient Descent(85/199): loss=0.812275831372251, w0=0.20101014429501818, w1=-0.11612238422910796\n",
      "Gradient Descent(86/199): loss=0.8121034831925678, w0=0.20125077751771261, w1=-0.11710054613145643\n",
      "Gradient Descent(87/199): loss=0.8119334398937904, w0=0.2014809229703721, w1=-0.11807641764428672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/199): loss=0.8117656486844305, w0=0.2017009923969502, w1=-0.11905004019754016\n",
      "Gradient Descent(89/199): loss=0.8116000588078969, w0=0.20191138135903594, w1=-0.12002145383897955\n",
      "Gradient Descent(90/199): loss=0.8114366214197636, w0=0.20211246987141904, w1=-0.12099069728950428\n",
      "Gradient Descent(91/199): loss=0.8112752894739303, w0=0.20230462301277688, w1=-0.12195780799598874\n",
      "Gradient Descent(92/199): loss=0.811116017616997, w0=0.202488191512448, w1=-0.12292282218177408\n",
      "Gradient Descent(93/199): loss=0.810958762090233, w0=0.2026635123142205, w1=-0.12388577489493513\n",
      "Gradient Descent(94/199): loss=0.8108034806385586, w0=0.2028309091180283, w1=-0.12484670005443589\n",
      "Gradient Descent(95/199): loss=0.8106501324260114, w0=0.20299069290041488, w1=-0.1258056304942805\n",
      "Gradient Descent(96/199): loss=0.8104986779572034, w0=0.20314316241459035, w1=-0.12676259800575945\n",
      "Gradient Descent(97/199): loss=0.810349079004314, w0=0.20328860467087728, w1=-0.12771763337788483\n",
      "Gradient Descent(98/199): loss=0.8102012985392067, w0=0.2034272953983094, w1=-0.12867076643610287\n",
      "Gradient Descent(99/199): loss=0.8100553006702761, w0=0.2035594994881185, w1=-0.12962202607936676\n",
      "Gradient Descent(100/199): loss=0.8099110505836685, w0=0.20368547141981636, w1=-0.13057144031564777\n",
      "Gradient Descent(101/199): loss=0.8097685144885571, w0=0.20380545567055144, w1=-0.13151903629595824\n",
      "Gradient Descent(102/199): loss=0.8096276595661505, w0=0.2039196871083939, w1=-0.132464840346956\n",
      "Gradient Descent(103/199): loss=0.8094884539221701, w0=0.20402839137017711, w1=-0.13340887800219578\n",
      "Gradient Descent(104/199): loss=0.8093508665425282, w0=0.20413178522449982, w1=-0.13435117403208963\n",
      "Gradient Descent(105/199): loss=0.8092148672519681, w0=0.20423007692046946, w1=-0.13529175247263475\n",
      "Gradient Descent(106/199): loss=0.8090804266754488, w0=0.20432346652274502, w1=-0.1362306366529646\n",
      "Gradient Descent(107/199): loss=0.8089475162020672, w0=0.20441214623341555, w1=-0.13716784922177552\n",
      "Gradient Descent(108/199): loss=0.8088161079513296, w0=0.20449630070123045, w1=-0.13810341217267913\n",
      "Gradient Descent(109/199): loss=0.8086861747416009, w0=0.20457610731867676, w1=-0.13903734686852717\n",
      "Gradient Descent(110/199): loss=0.8085576900605679, w0=0.20465173650738022, w1=-0.1399696740647545\n",
      "Gradient Descent(111/199): loss=0.8084306280375686, w0=0.20472335199228758, w1=-0.14090041393178218\n",
      "Gradient Descent(112/199): loss=0.8083049634176541, w0=0.20479111106507034, w1=-0.14182958607652174\n",
      "Gradient Descent(113/199): loss=0.80818067153725, w0=0.2048551648371726, w1=-0.1427572095630189\n",
      "Gradient Descent(114/199): loss=0.8080577283013065, w0=0.20491565848290968, w1=-0.14368330293227363\n",
      "Gradient Descent(115/199): loss=0.8079361101618251, w0=0.20497273147300762, w1=-0.14460788422127147\n",
      "Gradient Descent(116/199): loss=0.8078157940976656, w0=0.20502651779895903, w1=-0.14553097098125933\n",
      "Gradient Descent(117/199): loss=0.807696757595536, w0=0.20507714618855571, w1=-0.14645258029529784\n",
      "Gradient Descent(118/199): loss=0.8075789786320857, w0=0.2051247403129448, w1=-0.14737272879512012\n",
      "Gradient Descent(119/199): loss=0.8074624356570195, w0=0.20516941898554095, w1=-0.14829143267732617\n",
      "Gradient Descent(120/199): loss=0.8073471075771625, w0=0.20521129635311486, w1=-0.1492087077189405\n",
      "Gradient Descent(121/199): loss=0.8072329737414032, w0=0.20525048207936525, w1=-0.15012456929235898\n",
      "Gradient Descent(122/199): loss=0.8071200139264624, w0=0.20528708152126973, w1=-0.1510390323797106\n",
      "Gradient Descent(123/199): loss=0.8070082083234177, w0=0.20532119589849843, w1=-0.15195211158665767\n",
      "Gradient Descent(124/199): loss=0.8068975375249421, w0=0.2053529224561627, w1=-0.15286382115565775\n",
      "Gradient Descent(125/199): loss=0.8067879825132006, w0=0.2053823546211614, w1=-0.1537741749787093\n",
      "Gradient Descent(126/199): loss=0.8066795246483626, w0=0.20540958215237584, w1=-0.1546831866096019\n",
      "Gradient Descent(127/199): loss=0.8065721456576846, w0=0.20543469128495578, w1=-0.1555908692756912\n",
      "Gradient Descent(128/199): loss=0.8064658276251302, w0=0.20545776486892836, w1=-0.15649723588921802\n",
      "Gradient Descent(129/199): loss=0.806360552981483, w0=0.2054788825023535, w1=-0.15740229905818956\n",
      "Gradient Descent(130/199): loss=0.8062563044949304, w0=0.20549812065923997, w1=-0.15830607109684114\n",
      "Gradient Descent(131/199): loss=0.8061530652620735, w0=0.20551555281242845, w1=-0.1592085640356945\n",
      "Gradient Descent(132/199): loss=0.80605081869935, w0=0.20553124955163926, w1=-0.16010978963122938\n",
      "Gradient Descent(133/199): loss=0.80594954853483, w0=0.2055452786968751, w1=-0.16100975937518372\n",
      "Gradient Descent(134/199): loss=0.8058492388003674, w0=0.2055577054073615, w1=-0.1619084845034974\n",
      "Gradient Descent(135/199): loss=0.8057498738240877, w0=0.20556859228620036, w1=-0.1628059760049135\n",
      "Gradient Descent(136/199): loss=0.8056514382231773, w0=0.20557799948090544, w1=-0.16370224462925115\n",
      "Gradient Descent(137/199): loss=0.8055539168969752, w0=0.20558598477998166, w1=-0.16459730089536256\n",
      "Gradient Descent(138/199): loss=0.8054572950203286, w0=0.20559260370570384, w1=-0.16549115509878728\n",
      "Gradient Descent(139/199): loss=0.805361558037211, w0=0.20559790960324448, w1=-0.16638381731911522\n",
      "Gradient Descent(140/199): loss=0.8052666916545792, w0=0.20560195372629414, w1=-0.16727529742707034\n",
      "Gradient Descent(141/199): loss=0.805172681836457, w0=0.20560478531931264, w1=-0.16816560509132572\n",
      "Gradient Descent(142/199): loss=0.8050795147982346, w0=0.20560645169654324, w1=-0.16905474978506116\n",
      "Gradient Descent(143/199): loss=0.8049871770011647, w0=0.2056069983179176, w1=-0.16994274079227278\n",
      "Gradient Descent(144/199): loss=0.8048956551470537, w0=0.20560646886197353, w1=-0.17082958721384495\n",
      "Gradient Descent(145/199): loss=0.8048049361731294, w0=0.20560490529590345, w1=-0.1717152979733936\n",
      "Gradient Descent(146/199): loss=0.8047150072470751, w0=0.20560234794284593, w1=-0.17259988182289016\n",
      "Gradient Descent(147/199): loss=0.8046258557622293, w0=0.2055988355465294, w1=-0.1734833473480745\n",
      "Gradient Descent(148/199): loss=0.8045374693329317, w0=0.2055944053333718, w1=-0.1743657029736653\n",
      "Gradient Descent(149/199): loss=0.8044498357900159, w0=0.20558909307213644, w1=-0.17524695696837572\n",
      "Gradient Descent(150/199): loss=0.8043629431764361, w0=0.20558293313124024, w1=-0.17612711744974216\n",
      "Gradient Descent(151/199): loss=0.8042767797430243, w0=0.2055759585338069, w1=-0.1770061923887731\n",
      "Gradient Descent(152/199): loss=0.804191333944368, w0=0.20556820101055326, w1=-0.17788418961442556\n",
      "Gradient Descent(153/199): loss=0.804106594434808, w0=0.2055596910505948, w1=-0.17876111681791532\n",
      "Gradient Descent(154/199): loss=0.8040225500645433, w0=0.20555045795025162, w1=-0.1796369815568679\n",
      "Gradient Descent(155/199): loss=0.8039391898758456, w0=0.20554052985993387, w1=-0.18051179125931624\n",
      "Gradient Descent(156/199): loss=0.8038565030993707, w0=0.20552993382918217, w1=-0.18138555322755123\n",
      "Gradient Descent(157/199): loss=0.8037744791505712, w0=0.20551869584993554, w1=-0.1822582746418306\n",
      "Gradient Descent(158/199): loss=0.8036931076261975, w0=0.20550684089809687, w1=-0.18312996256395173\n",
      "Gradient Descent(159/199): loss=0.8036123783008865, w0=0.2054943929734625, w1=-0.18400062394069408\n",
      "Gradient Descent(160/199): loss=0.80353228112384, w0=0.20548137513808076, w1=-0.1848702656071356\n",
      "Gradient Descent(161/199): loss=0.803452806215578, w0=0.205467809553101, w1=-0.18573889428984866\n",
      "Gradient Descent(162/199): loss=0.8033739438647729, w0=0.2054537175141726, w1=-0.18660651660997973\n",
      "Gradient Descent(163/199): loss=0.8032956845251581, w0=0.20543911948545107, w1=-0.18747313908621765\n",
      "Gradient Descent(164/199): loss=0.8032180188125072, w0=0.20542403513226606, w1=-0.1883387681376545\n",
      "Gradient Descent(165/199): loss=0.803140937501683, w0=0.20540848335250392, w1=-0.18920341008654343\n",
      "Gradient Descent(166/199): loss=0.8030644315237542, w0=0.20539248230675541, w1=-0.19006707116095753\n",
      "Gradient Descent(167/199): loss=0.8029884919631732, w0=0.20537604944727714, w1=-0.19092975749735308\n",
      "Gradient Descent(168/199): loss=0.8029131100550191, w0=0.20535920154581352, w1=-0.19179147514304162\n",
      "Gradient Descent(169/199): loss=0.8028382771822958, w0=0.20534195472032396, w1=-0.19265223005857363\n",
      "Gradient Descent(170/199): loss=0.8027639848732953, w0=0.20532432446065849, w1=-0.19351202812003762\n",
      "Gradient Descent(171/199): loss=0.8026902247990081, w0=0.20530632565322318, w1=-0.19437087512127801\n",
      "Gradient Descent(172/199): loss=0.802616988770593, w0=0.20528797260467502, w1=-0.19522877677603442\n",
      "Gradient Descent(173/199): loss=0.8025442687368994, w0=0.20526927906468467, w1=-0.19608573872000587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(174/199): loss=0.802472056782036, w0=0.20525025824780352, w1=-0.1969417665128427\n",
      "Gradient Descent(175/199): loss=0.8024003451229933, w0=0.20523092285447053, w1=-0.19779686564006865\n",
      "Gradient Descent(176/199): loss=0.802329126107312, w0=0.2052112850911926, w1=-0.1986510415149362\n",
      "Gradient Descent(177/199): loss=0.8022583922107956, w0=0.20519135668993094, w1=-0.19950429948021756\n",
      "Gradient Descent(178/199): loss=0.8021881360352722, w0=0.20517114892672492, w1=-0.2003566448099335\n",
      "Gradient Descent(179/199): loss=0.8021183503063969, w0=0.20515067263958311, w1=-0.20120808271102308\n",
      "Gradient Descent(180/199): loss=0.802049027871498, w0=0.20512993824567058, w1=-0.20205861832495567\n",
      "Gradient Descent(181/199): loss=0.8019801616974632, w0=0.20510895575782012, w1=-0.20290825672928822\n",
      "Gradient Descent(182/199): loss=0.8019117448686697, w0=0.20508773480039386, w1=-0.20375700293916943\n",
      "Gradient Descent(183/199): loss=0.8018437705849478, w0=0.20506628462452106, w1=-0.20460486190879293\n",
      "Gradient Descent(184/199): loss=0.80177623215959, w0=0.20504461412273642, w1=-0.20545183853280147\n",
      "Gradient Descent(185/199): loss=0.801709123017391, w0=0.20502273184304265, w1=-0.20629793764764404\n",
      "Gradient Descent(186/199): loss=0.8016424366927276, w0=0.20500064600242, w1=-0.20714316403288743\n",
      "Gradient Descent(187/199): loss=0.8015761668276742, w0=0.2049783644998042, w1=-0.20798752241248436\n",
      "Gradient Descent(188/199): loss=0.8015103071701509, w0=0.20495589492855426, w1=-0.20883101745599938\n",
      "Gradient Descent(189/199): loss=0.801444851572109, w0=0.20493324458842968, w1=-0.2096736537797946\n",
      "Gradient Descent(190/199): loss=0.8013797939877448, w0=0.2049104204970967, w1=-0.21051543594817637\n",
      "Gradient Descent(191/199): loss=0.8013151284717515, w0=0.20488742940118204, w1=-0.2113563684745045\n",
      "Gradient Descent(192/199): loss=0.8012508491775981, w0=0.20486427778689195, w1=-0.2121964558222657\n",
      "Gradient Descent(193/199): loss=0.801186950355841, w0=0.2048409718902136, w1=-0.2130357024061121\n",
      "Gradient Descent(194/199): loss=0.8011234263524682, w0=0.20481751770671547, w1=-0.21387411259286662\n",
      "Gradient Descent(195/199): loss=0.8010602716072688, w0=0.20479392100096208, w1=-0.21471169070249607\n",
      "Gradient Descent(196/199): loss=0.8009974806522341, w0=0.2047701873155589, w1=-0.2155484410090534\n",
      "Gradient Descent(197/199): loss=0.8009350481099886, w0=0.20474632197984127, w1=-0.21638436774159037\n",
      "Gradient Descent(198/199): loss=0.800872968692245, w0=0.20472233011822197, w1=-0.21721947508504125\n",
      "Gradient Descent(199/199): loss=0.8008112371982898, w0=0.20469821665821036, w1=-0.21805376718107922\n",
      "Gradient Descent: execution time=8.186 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.05\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "stoch Gradient Descent(0/499): loss=1.0\n",
      "stoch Gradient Descent(1/499): loss=0.8748329763372845\n",
      "stoch Gradient Descent(2/499): loss=0.8750850686707375\n",
      "stoch Gradient Descent(3/499): loss=0.8686686036034618\n",
      "stoch Gradient Descent(4/499): loss=0.8608679148802753\n",
      "stoch Gradient Descent(5/499): loss=0.8607693417990282\n",
      "stoch Gradient Descent(6/499): loss=0.8645060672515698\n",
      "stoch Gradient Descent(7/499): loss=0.8604048066595569\n",
      "stoch Gradient Descent(8/499): loss=0.8532792426369813\n",
      "stoch Gradient Descent(9/499): loss=0.8575215281097782\n",
      "stoch Gradient Descent(10/499): loss=0.8485336421575519\n",
      "stoch Gradient Descent(11/499): loss=0.8520513621131535\n",
      "stoch Gradient Descent(12/499): loss=0.8395967875366114\n",
      "stoch Gradient Descent(13/499): loss=0.8488755017591948\n",
      "stoch Gradient Descent(14/499): loss=0.8539659247005784\n",
      "stoch Gradient Descent(15/499): loss=0.8450843544715863\n",
      "stoch Gradient Descent(16/499): loss=0.8462207705527477\n",
      "stoch Gradient Descent(17/499): loss=0.8476228115966231\n",
      "stoch Gradient Descent(18/499): loss=0.8545183168318014\n",
      "stoch Gradient Descent(19/499): loss=0.8384895718806944\n",
      "stoch Gradient Descent(20/499): loss=0.8280125573851861\n",
      "stoch Gradient Descent(21/499): loss=0.8309526921368624\n",
      "stoch Gradient Descent(22/499): loss=0.8419506838855163\n",
      "stoch Gradient Descent(23/499): loss=0.8326421976789383\n",
      "stoch Gradient Descent(24/499): loss=0.826398062865214\n",
      "stoch Gradient Descent(25/499): loss=0.8272533923201785\n",
      "stoch Gradient Descent(26/499): loss=0.8283559278789917\n",
      "stoch Gradient Descent(27/499): loss=0.8285870907799991\n",
      "stoch Gradient Descent(28/499): loss=0.8292085401824876\n",
      "stoch Gradient Descent(29/499): loss=0.83136801480139\n",
      "stoch Gradient Descent(30/499): loss=0.8311398324301079\n",
      "stoch Gradient Descent(31/499): loss=0.8299782883395576\n",
      "stoch Gradient Descent(32/499): loss=0.8441655929547974\n",
      "stoch Gradient Descent(33/499): loss=0.8269810199798411\n",
      "stoch Gradient Descent(34/499): loss=0.8215554541810676\n",
      "stoch Gradient Descent(35/499): loss=0.830915250412604\n",
      "stoch Gradient Descent(36/499): loss=0.8269027444048128\n",
      "stoch Gradient Descent(37/499): loss=0.8301357418145914\n",
      "stoch Gradient Descent(38/499): loss=0.8298018176548067\n",
      "stoch Gradient Descent(39/499): loss=0.8152716295465965\n",
      "stoch Gradient Descent(40/499): loss=0.8226448822530874\n",
      "stoch Gradient Descent(41/499): loss=0.8265864492038905\n",
      "stoch Gradient Descent(42/499): loss=0.8167872147341135\n",
      "stoch Gradient Descent(43/499): loss=0.8262944599720968\n",
      "stoch Gradient Descent(44/499): loss=0.8134207411420309\n",
      "stoch Gradient Descent(45/499): loss=0.8268086245581034\n",
      "stoch Gradient Descent(46/499): loss=0.8177432635543868\n",
      "stoch Gradient Descent(47/499): loss=0.8176370648626413\n",
      "stoch Gradient Descent(48/499): loss=0.8205824135426566\n",
      "stoch Gradient Descent(49/499): loss=0.8182878905047771\n",
      "stoch Gradient Descent(50/499): loss=0.8124921793341616\n",
      "stoch Gradient Descent(51/499): loss=0.8175034128986416\n",
      "stoch Gradient Descent(52/499): loss=0.8247551992019518\n",
      "stoch Gradient Descent(53/499): loss=0.8235190454194379\n",
      "stoch Gradient Descent(54/499): loss=0.8173225461735029\n",
      "stoch Gradient Descent(55/499): loss=0.8068137046679084\n",
      "stoch Gradient Descent(56/499): loss=0.8283529519416883\n",
      "stoch Gradient Descent(57/499): loss=0.8212965827868375\n",
      "stoch Gradient Descent(58/499): loss=0.8088567415204804\n",
      "stoch Gradient Descent(59/499): loss=0.8120374359709783\n",
      "stoch Gradient Descent(60/499): loss=0.8302306181698186\n",
      "stoch Gradient Descent(61/499): loss=0.8174162754173095\n",
      "stoch Gradient Descent(62/499): loss=0.8171519592760502\n",
      "stoch Gradient Descent(63/499): loss=0.8169164198959273\n",
      "stoch Gradient Descent(64/499): loss=0.8172210182628371\n",
      "stoch Gradient Descent(65/499): loss=0.8210956928218253\n",
      "stoch Gradient Descent(66/499): loss=0.8142728481675101\n",
      "stoch Gradient Descent(67/499): loss=0.8128627604925674\n",
      "stoch Gradient Descent(68/499): loss=0.8101600186866474\n",
      "stoch Gradient Descent(69/499): loss=0.8172662107034208\n",
      "stoch Gradient Descent(70/499): loss=0.819732048404618\n",
      "stoch Gradient Descent(71/499): loss=0.816190313695673\n",
      "stoch Gradient Descent(72/499): loss=0.8130867759667432\n",
      "stoch Gradient Descent(73/499): loss=0.8093407243814361\n",
      "stoch Gradient Descent(74/499): loss=0.8155044422474084\n",
      "stoch Gradient Descent(75/499): loss=0.8139136457540093\n",
      "stoch Gradient Descent(76/499): loss=0.8119952755892177\n",
      "stoch Gradient Descent(77/499): loss=0.805793181392286\n",
      "stoch Gradient Descent(78/499): loss=0.8073037407328633\n",
      "stoch Gradient Descent(79/499): loss=0.8116987658210634\n",
      "stoch Gradient Descent(80/499): loss=0.8102414269483926\n",
      "stoch Gradient Descent(81/499): loss=0.8076688827192513\n",
      "stoch Gradient Descent(82/499): loss=0.8124107226090187\n",
      "stoch Gradient Descent(83/499): loss=0.813847420176431\n",
      "stoch Gradient Descent(84/499): loss=0.8049369572240839\n",
      "stoch Gradient Descent(85/499): loss=0.8125196813904485\n",
      "stoch Gradient Descent(86/499): loss=0.8203041361353276\n",
      "stoch Gradient Descent(87/499): loss=0.8162909977108882\n",
      "stoch Gradient Descent(88/499): loss=0.8070181440542061\n",
      "stoch Gradient Descent(89/499): loss=0.803652492996675\n",
      "stoch Gradient Descent(90/499): loss=0.8034699538872331\n",
      "stoch Gradient Descent(91/499): loss=0.8136984326289758\n",
      "stoch Gradient Descent(92/499): loss=0.8101473058612024\n",
      "stoch Gradient Descent(93/499): loss=0.8157967368212102\n",
      "stoch Gradient Descent(94/499): loss=0.8139560341693078\n",
      "stoch Gradient Descent(95/499): loss=0.8076756678795411\n",
      "stoch Gradient Descent(96/499): loss=0.8132080922596768\n",
      "stoch Gradient Descent(97/499): loss=0.8034790199164001\n",
      "stoch Gradient Descent(98/499): loss=0.8127694357298051\n",
      "stoch Gradient Descent(99/499): loss=0.7947793381254558\n",
      "stoch Gradient Descent(100/499): loss=0.7994074287230506\n",
      "stoch Gradient Descent(101/499): loss=0.7975563391198208\n",
      "stoch Gradient Descent(102/499): loss=0.8162062715566218\n",
      "stoch Gradient Descent(103/499): loss=0.8060928222510092\n",
      "stoch Gradient Descent(104/499): loss=0.8084444324763664\n",
      "stoch Gradient Descent(105/499): loss=0.8121057306308316\n",
      "stoch Gradient Descent(106/499): loss=0.8006361621656864\n",
      "stoch Gradient Descent(107/499): loss=0.812047258018054\n",
      "stoch Gradient Descent(108/499): loss=0.8122713565817882\n",
      "stoch Gradient Descent(109/499): loss=0.8164381967505542\n",
      "stoch Gradient Descent(110/499): loss=0.8050034187880267\n",
      "stoch Gradient Descent(111/499): loss=0.7997584969802072\n",
      "stoch Gradient Descent(112/499): loss=0.8129346055687175\n",
      "stoch Gradient Descent(113/499): loss=0.8072556424469463\n",
      "stoch Gradient Descent(114/499): loss=0.8085036341352787\n",
      "stoch Gradient Descent(115/499): loss=0.8057350381202093\n",
      "stoch Gradient Descent(116/499): loss=0.797754134382965\n",
      "stoch Gradient Descent(117/499): loss=0.8060480147714492\n",
      "stoch Gradient Descent(118/499): loss=0.7986947961456301\n",
      "stoch Gradient Descent(119/499): loss=0.8034182404620287\n",
      "stoch Gradient Descent(120/499): loss=0.8140157164648101\n",
      "stoch Gradient Descent(121/499): loss=0.8017778762557441\n",
      "stoch Gradient Descent(122/499): loss=0.8082452742304996\n",
      "stoch Gradient Descent(123/499): loss=0.7974371291014991\n",
      "stoch Gradient Descent(124/499): loss=0.8150485735277274\n",
      "stoch Gradient Descent(125/499): loss=0.8135310063332063\n",
      "stoch Gradient Descent(126/499): loss=0.8054480578367724\n",
      "stoch Gradient Descent(127/499): loss=0.8039080891040716\n",
      "stoch Gradient Descent(128/499): loss=0.8072973443629894\n",
      "stoch Gradient Descent(129/499): loss=0.8021024933243235\n",
      "stoch Gradient Descent(130/499): loss=0.8090263079398893\n",
      "stoch Gradient Descent(131/499): loss=0.8007994361994192\n",
      "stoch Gradient Descent(132/499): loss=0.8046224497172967\n",
      "stoch Gradient Descent(133/499): loss=0.8134182892487136\n",
      "stoch Gradient Descent(134/499): loss=0.8165573648604779\n",
      "stoch Gradient Descent(135/499): loss=0.8099077136369884\n",
      "stoch Gradient Descent(136/499): loss=0.8123946234490766\n",
      "stoch Gradient Descent(137/499): loss=0.7974611123159339\n",
      "stoch Gradient Descent(138/499): loss=0.7960119439535032\n",
      "stoch Gradient Descent(139/499): loss=0.8006114241312388\n",
      "stoch Gradient Descent(140/499): loss=0.8108294887795386\n",
      "stoch Gradient Descent(141/499): loss=0.8092958217514331\n",
      "stoch Gradient Descent(142/499): loss=0.8114906739159701\n",
      "stoch Gradient Descent(143/499): loss=0.798911740938471\n",
      "stoch Gradient Descent(144/499): loss=0.7939624111078487\n",
      "stoch Gradient Descent(145/499): loss=0.8051509554959942\n",
      "stoch Gradient Descent(146/499): loss=0.8070288898848558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoch Gradient Descent(147/499): loss=0.8061374915319588\n",
      "stoch Gradient Descent(148/499): loss=0.8057495594774132\n",
      "stoch Gradient Descent(149/499): loss=0.8087177981046925\n",
      "stoch Gradient Descent(150/499): loss=0.7982180081773258\n",
      "stoch Gradient Descent(151/499): loss=0.7984030151537261\n",
      "stoch Gradient Descent(152/499): loss=0.8011568911031891\n",
      "stoch Gradient Descent(153/499): loss=0.8063763864696846\n",
      "stoch Gradient Descent(154/499): loss=0.8061577064439022\n",
      "stoch Gradient Descent(155/499): loss=0.7963395454379539\n",
      "stoch Gradient Descent(156/499): loss=0.7966018660329781\n",
      "stoch Gradient Descent(157/499): loss=0.806803437401804\n",
      "stoch Gradient Descent(158/499): loss=0.8118941246472636\n",
      "stoch Gradient Descent(159/499): loss=0.7947371277242355\n",
      "stoch Gradient Descent(160/499): loss=0.8046814947530171\n",
      "stoch Gradient Descent(161/499): loss=0.8048720667967942\n",
      "stoch Gradient Descent(162/499): loss=0.8090859474777933\n",
      "stoch Gradient Descent(163/499): loss=0.794012111703444\n",
      "stoch Gradient Descent(164/499): loss=0.7984166077420607\n",
      "stoch Gradient Descent(165/499): loss=0.8049552445357419\n",
      "stoch Gradient Descent(166/499): loss=0.8141095205172129\n",
      "stoch Gradient Descent(167/499): loss=0.8040607790115158\n",
      "stoch Gradient Descent(168/499): loss=0.8075327294990677\n",
      "stoch Gradient Descent(169/499): loss=0.8053624530957109\n",
      "stoch Gradient Descent(170/499): loss=0.7978249135522635\n",
      "stoch Gradient Descent(171/499): loss=0.8006540481705973\n",
      "stoch Gradient Descent(172/499): loss=0.8041065440184659\n",
      "stoch Gradient Descent(173/499): loss=0.8022478961613647\n",
      "stoch Gradient Descent(174/499): loss=0.7990728813323515\n",
      "stoch Gradient Descent(175/499): loss=0.7927412048938368\n",
      "stoch Gradient Descent(176/499): loss=0.8081927568722964\n",
      "stoch Gradient Descent(177/499): loss=0.8024615682448294\n",
      "stoch Gradient Descent(178/499): loss=0.8061361595470394\n",
      "stoch Gradient Descent(179/499): loss=0.7980668069450607\n",
      "stoch Gradient Descent(180/499): loss=0.7976926581510366\n",
      "stoch Gradient Descent(181/499): loss=0.8046686063987422\n",
      "stoch Gradient Descent(182/499): loss=0.8070033246694145\n",
      "stoch Gradient Descent(183/499): loss=0.7989850559334852\n",
      "stoch Gradient Descent(184/499): loss=0.7940188722671918\n",
      "stoch Gradient Descent(185/499): loss=0.7963064760395506\n",
      "stoch Gradient Descent(186/499): loss=0.7964619588127388\n",
      "stoch Gradient Descent(187/499): loss=0.8055241487834197\n",
      "stoch Gradient Descent(188/499): loss=0.7971576267645887\n",
      "stoch Gradient Descent(189/499): loss=0.8041122300233797\n",
      "stoch Gradient Descent(190/499): loss=0.7994889775052361\n",
      "stoch Gradient Descent(191/499): loss=0.8054781857416554\n",
      "stoch Gradient Descent(192/499): loss=0.7973050264414882\n",
      "stoch Gradient Descent(193/499): loss=0.8031518189134796\n",
      "stoch Gradient Descent(194/499): loss=0.8040687559001994\n",
      "stoch Gradient Descent(195/499): loss=0.7952841199510096\n",
      "stoch Gradient Descent(196/499): loss=0.8065595929555579\n",
      "stoch Gradient Descent(197/499): loss=0.8004110400959586\n",
      "stoch Gradient Descent(198/499): loss=0.8091933830827313\n",
      "stoch Gradient Descent(199/499): loss=0.7832543503997593\n",
      "stoch Gradient Descent(200/499): loss=0.7886105451409432\n",
      "stoch Gradient Descent(201/499): loss=0.7967149260803464\n",
      "stoch Gradient Descent(202/499): loss=0.807769502923226\n",
      "stoch Gradient Descent(203/499): loss=0.8047844842728041\n",
      "stoch Gradient Descent(204/499): loss=0.7992932700106303\n",
      "stoch Gradient Descent(205/499): loss=0.7988988682045352\n",
      "stoch Gradient Descent(206/499): loss=0.7987385422547172\n",
      "stoch Gradient Descent(207/499): loss=0.7954583828883252\n",
      "stoch Gradient Descent(208/499): loss=0.798889410078403\n",
      "stoch Gradient Descent(209/499): loss=0.8134229126296606\n",
      "stoch Gradient Descent(210/499): loss=0.7959152988896214\n",
      "stoch Gradient Descent(211/499): loss=0.8143007038071751\n",
      "stoch Gradient Descent(212/499): loss=0.8050257083052922\n",
      "stoch Gradient Descent(213/499): loss=0.8024281728709886\n",
      "stoch Gradient Descent(214/499): loss=0.7998853214769422\n",
      "stoch Gradient Descent(215/499): loss=0.8024637898111783\n",
      "stoch Gradient Descent(216/499): loss=0.7983065725224476\n",
      "stoch Gradient Descent(217/499): loss=0.8017866564224145\n",
      "stoch Gradient Descent(218/499): loss=0.7963898439651448\n",
      "stoch Gradient Descent(219/499): loss=0.7972332901912079\n",
      "stoch Gradient Descent(220/499): loss=0.8051414404294779\n",
      "stoch Gradient Descent(221/499): loss=0.7844311649162471\n",
      "stoch Gradient Descent(222/499): loss=0.8014258066452503\n",
      "stoch Gradient Descent(223/499): loss=0.7988540127536496\n",
      "stoch Gradient Descent(224/499): loss=0.7955495154801284\n",
      "stoch Gradient Descent(225/499): loss=0.7876097841450785\n",
      "stoch Gradient Descent(226/499): loss=0.7946392038616891\n",
      "stoch Gradient Descent(227/499): loss=0.8078398067676713\n",
      "stoch Gradient Descent(228/499): loss=0.7996578385363607\n",
      "stoch Gradient Descent(229/499): loss=0.7925157735274487\n",
      "stoch Gradient Descent(230/499): loss=0.7937013863599476\n",
      "stoch Gradient Descent(231/499): loss=0.7989113774631396\n",
      "stoch Gradient Descent(232/499): loss=0.7936324937760104\n",
      "stoch Gradient Descent(233/499): loss=0.7912906994763876\n",
      "stoch Gradient Descent(234/499): loss=0.8020429910014937\n",
      "stoch Gradient Descent(235/499): loss=0.790742880657299\n",
      "stoch Gradient Descent(236/499): loss=0.8007950333380847\n",
      "stoch Gradient Descent(237/499): loss=0.7999502859183006\n",
      "stoch Gradient Descent(238/499): loss=0.7935204140886238\n",
      "stoch Gradient Descent(239/499): loss=0.7869645855643457\n",
      "stoch Gradient Descent(240/499): loss=0.7882182576686202\n",
      "stoch Gradient Descent(241/499): loss=0.8050800094549992\n",
      "stoch Gradient Descent(242/499): loss=0.7972644211814238\n",
      "stoch Gradient Descent(243/499): loss=0.8050998370840411\n",
      "stoch Gradient Descent(244/499): loss=0.7984826218557143\n",
      "stoch Gradient Descent(245/499): loss=0.807354440783133\n",
      "stoch Gradient Descent(246/499): loss=0.7950707576604744\n",
      "stoch Gradient Descent(247/499): loss=0.7968117879304634\n",
      "stoch Gradient Descent(248/499): loss=0.7944886425303356\n",
      "stoch Gradient Descent(249/499): loss=0.8043036148480021\n",
      "stoch Gradient Descent(250/499): loss=0.8049953532846604\n",
      "stoch Gradient Descent(251/499): loss=0.8043509533508009\n",
      "stoch Gradient Descent(252/499): loss=0.7864501553773318\n",
      "stoch Gradient Descent(253/499): loss=0.7844784951686453\n",
      "stoch Gradient Descent(254/499): loss=0.8030511220554953\n",
      "stoch Gradient Descent(255/499): loss=0.7989798245900549\n",
      "stoch Gradient Descent(256/499): loss=0.7969886890066407\n",
      "stoch Gradient Descent(257/499): loss=0.7833993321760278\n",
      "stoch Gradient Descent(258/499): loss=0.7945878762723193\n",
      "stoch Gradient Descent(259/499): loss=0.799852002711147\n",
      "stoch Gradient Descent(260/499): loss=0.8047834156797622\n",
      "stoch Gradient Descent(261/499): loss=0.7975525199413264\n",
      "stoch Gradient Descent(262/499): loss=0.8027207193688922\n",
      "stoch Gradient Descent(263/499): loss=0.7978030852855589\n",
      "stoch Gradient Descent(264/499): loss=0.7991365838040592\n",
      "stoch Gradient Descent(265/499): loss=0.7877925729308322\n",
      "stoch Gradient Descent(266/499): loss=0.808876313810469\n",
      "stoch Gradient Descent(267/499): loss=0.8047217694452813\n",
      "stoch Gradient Descent(268/499): loss=0.795173410367992\n",
      "stoch Gradient Descent(269/499): loss=0.8007791516718358\n",
      "stoch Gradient Descent(270/499): loss=0.7860100853221915\n",
      "stoch Gradient Descent(271/499): loss=0.7911787673180841\n",
      "stoch Gradient Descent(272/499): loss=0.7970902502421339\n",
      "stoch Gradient Descent(273/499): loss=0.7915528880160374\n",
      "stoch Gradient Descent(274/499): loss=0.80374020085824\n",
      "stoch Gradient Descent(275/499): loss=0.8083562599111223\n",
      "stoch Gradient Descent(276/499): loss=0.8025746843561292\n",
      "stoch Gradient Descent(277/499): loss=0.7946853431573269\n",
      "stoch Gradient Descent(278/499): loss=0.7964105586735357\n",
      "stoch Gradient Descent(279/499): loss=0.7934718193772132\n",
      "stoch Gradient Descent(280/499): loss=0.8005653889185893\n",
      "stoch Gradient Descent(281/499): loss=0.8123196386010054\n",
      "stoch Gradient Descent(282/499): loss=0.7899601166732291\n",
      "stoch Gradient Descent(283/499): loss=0.8006296968100907\n",
      "stoch Gradient Descent(284/499): loss=0.7938026442409507\n",
      "stoch Gradient Descent(285/499): loss=0.8025290305278502\n",
      "stoch Gradient Descent(286/499): loss=0.7898872445659832\n",
      "stoch Gradient Descent(287/499): loss=0.7849853471051074\n",
      "stoch Gradient Descent(288/499): loss=0.793280373176811\n",
      "stoch Gradient Descent(289/499): loss=0.7866883486993519\n",
      "stoch Gradient Descent(290/499): loss=0.7941066715205799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoch Gradient Descent(291/499): loss=0.7918754921128127\n",
      "stoch Gradient Descent(292/499): loss=0.7947102942921181\n",
      "stoch Gradient Descent(293/499): loss=0.7970729833823954\n",
      "stoch Gradient Descent(294/499): loss=0.7925503747669687\n",
      "stoch Gradient Descent(295/499): loss=0.7964708537209197\n",
      "stoch Gradient Descent(296/499): loss=0.7882957711192141\n",
      "stoch Gradient Descent(297/499): loss=0.7973246855470593\n",
      "stoch Gradient Descent(298/499): loss=0.7932278908118691\n",
      "stoch Gradient Descent(299/499): loss=0.7949484684293783\n",
      "stoch Gradient Descent(300/499): loss=0.7970251341977042\n",
      "stoch Gradient Descent(301/499): loss=0.7916818818127959\n",
      "stoch Gradient Descent(302/499): loss=0.7938200634785672\n",
      "stoch Gradient Descent(303/499): loss=0.8042259855063839\n",
      "stoch Gradient Descent(304/499): loss=0.7922766011903123\n",
      "stoch Gradient Descent(305/499): loss=0.8007922907847418\n",
      "stoch Gradient Descent(306/499): loss=0.8058251280523876\n",
      "stoch Gradient Descent(307/499): loss=0.7963160078009652\n",
      "stoch Gradient Descent(308/499): loss=0.7948897199344765\n",
      "stoch Gradient Descent(309/499): loss=0.8093255803823699\n",
      "stoch Gradient Descent(310/499): loss=0.7990407091422167\n",
      "stoch Gradient Descent(311/499): loss=0.7917670746304534\n",
      "stoch Gradient Descent(312/499): loss=0.7887449634818339\n",
      "stoch Gradient Descent(313/499): loss=0.802059233777038\n",
      "stoch Gradient Descent(314/499): loss=0.79215321908418\n",
      "stoch Gradient Descent(315/499): loss=0.7979697986811037\n",
      "stoch Gradient Descent(316/499): loss=0.7890847509938137\n",
      "stoch Gradient Descent(317/499): loss=0.7988311092537141\n",
      "stoch Gradient Descent(318/499): loss=0.7917679688173069\n",
      "stoch Gradient Descent(319/499): loss=0.78751678517595\n",
      "stoch Gradient Descent(320/499): loss=0.795385808785938\n",
      "stoch Gradient Descent(321/499): loss=0.7970624356805928\n",
      "stoch Gradient Descent(322/499): loss=0.798187434694406\n",
      "stoch Gradient Descent(323/499): loss=0.7966630908669476\n",
      "stoch Gradient Descent(324/499): loss=0.7915020145221715\n",
      "stoch Gradient Descent(325/499): loss=0.7951738546928934\n",
      "stoch Gradient Descent(326/499): loss=0.794854006671193\n",
      "stoch Gradient Descent(327/499): loss=0.8103603575644244\n",
      "stoch Gradient Descent(328/499): loss=0.7911062314958084\n",
      "stoch Gradient Descent(329/499): loss=0.7900507185236232\n",
      "stoch Gradient Descent(330/499): loss=0.7930879926283302\n",
      "stoch Gradient Descent(331/499): loss=0.7951536854410398\n",
      "stoch Gradient Descent(332/499): loss=0.7946255760104088\n",
      "stoch Gradient Descent(333/499): loss=0.7882905926612904\n",
      "stoch Gradient Descent(334/499): loss=0.7811742831010079\n",
      "stoch Gradient Descent(335/499): loss=0.7963253897717217\n",
      "stoch Gradient Descent(336/499): loss=0.7931248575151846\n",
      "stoch Gradient Descent(337/499): loss=0.8038440037320286\n",
      "stoch Gradient Descent(338/499): loss=0.7915459415869154\n",
      "stoch Gradient Descent(339/499): loss=0.7869876822847386\n",
      "stoch Gradient Descent(340/499): loss=0.7911776961125053\n",
      "stoch Gradient Descent(341/499): loss=0.7842134215144719\n",
      "stoch Gradient Descent(342/499): loss=0.7917115533601966\n",
      "stoch Gradient Descent(343/499): loss=0.7970239506994774\n",
      "stoch Gradient Descent(344/499): loss=0.8035069705337251\n",
      "stoch Gradient Descent(345/499): loss=0.7914985657671103\n",
      "stoch Gradient Descent(346/499): loss=0.8010106424866849\n",
      "stoch Gradient Descent(347/499): loss=0.7898444954062468\n",
      "stoch Gradient Descent(348/499): loss=0.7906882926237395\n",
      "stoch Gradient Descent(349/499): loss=0.7952512841255241\n",
      "stoch Gradient Descent(350/499): loss=0.7881076589017266\n",
      "stoch Gradient Descent(351/499): loss=0.7887997527827595\n",
      "stoch Gradient Descent(352/499): loss=0.7928956406436537\n",
      "stoch Gradient Descent(353/499): loss=0.7865152609923676\n",
      "stoch Gradient Descent(354/499): loss=0.7920223300184238\n",
      "stoch Gradient Descent(355/499): loss=0.7945315213459978\n",
      "stoch Gradient Descent(356/499): loss=0.7939126694742579\n",
      "stoch Gradient Descent(357/499): loss=0.7930700983593798\n",
      "stoch Gradient Descent(358/499): loss=0.7916091867118206\n",
      "stoch Gradient Descent(359/499): loss=0.794107969583114\n",
      "stoch Gradient Descent(360/499): loss=0.7855526897690179\n",
      "stoch Gradient Descent(361/499): loss=0.7998959770721018\n",
      "stoch Gradient Descent(362/499): loss=0.7902514217387845\n",
      "stoch Gradient Descent(363/499): loss=0.7931505849008856\n",
      "stoch Gradient Descent(364/499): loss=0.8008326099950688\n",
      "stoch Gradient Descent(365/499): loss=0.7951002375073107\n",
      "stoch Gradient Descent(366/499): loss=0.7934603818453234\n",
      "stoch Gradient Descent(367/499): loss=0.791174662153245\n",
      "stoch Gradient Descent(368/499): loss=0.7896380610702688\n",
      "stoch Gradient Descent(369/499): loss=0.7894414780599033\n",
      "stoch Gradient Descent(370/499): loss=0.7934284784445849\n",
      "stoch Gradient Descent(371/499): loss=0.7905550236559583\n",
      "stoch Gradient Descent(372/499): loss=0.7940170727560334\n",
      "stoch Gradient Descent(373/499): loss=0.7838990328718317\n",
      "stoch Gradient Descent(374/499): loss=0.7991486337395725\n",
      "stoch Gradient Descent(375/499): loss=0.7901769577373644\n",
      "stoch Gradient Descent(376/499): loss=0.7877230094391099\n",
      "stoch Gradient Descent(377/499): loss=0.7921274435652773\n",
      "stoch Gradient Descent(378/499): loss=0.7918425444321097\n",
      "stoch Gradient Descent(379/499): loss=0.7878931135025014\n",
      "stoch Gradient Descent(380/499): loss=0.7866482439699396\n",
      "stoch Gradient Descent(381/499): loss=0.7908621219752988\n",
      "stoch Gradient Descent(382/499): loss=0.7904727638093892\n",
      "stoch Gradient Descent(383/499): loss=0.8023005751698221\n",
      "stoch Gradient Descent(384/499): loss=0.7941310238021099\n",
      "stoch Gradient Descent(385/499): loss=0.7922483481427355\n",
      "stoch Gradient Descent(386/499): loss=0.795531158372592\n",
      "stoch Gradient Descent(387/499): loss=0.7993675005259134\n",
      "stoch Gradient Descent(388/499): loss=0.7932106519244825\n",
      "stoch Gradient Descent(389/499): loss=0.7917420486023222\n",
      "stoch Gradient Descent(390/499): loss=0.8013625979607467\n",
      "stoch Gradient Descent(391/499): loss=0.785259693387838\n",
      "stoch Gradient Descent(392/499): loss=0.7931155299257928\n",
      "stoch Gradient Descent(393/499): loss=0.7895573342497133\n",
      "stoch Gradient Descent(394/499): loss=0.7959794886668112\n",
      "stoch Gradient Descent(395/499): loss=0.7918711579004155\n",
      "stoch Gradient Descent(396/499): loss=0.7932725495559022\n",
      "stoch Gradient Descent(397/499): loss=0.7933191660820801\n",
      "stoch Gradient Descent(398/499): loss=0.7932905672860855\n",
      "stoch Gradient Descent(399/499): loss=0.8002446106012697\n",
      "stoch Gradient Descent(400/499): loss=0.7992318195773407\n",
      "stoch Gradient Descent(401/499): loss=0.7881451920206388\n",
      "stoch Gradient Descent(402/499): loss=0.795749740427392\n",
      "stoch Gradient Descent(403/499): loss=0.7932567646163505\n",
      "stoch Gradient Descent(404/499): loss=0.7840688658083841\n",
      "stoch Gradient Descent(405/499): loss=0.7833567471317808\n",
      "stoch Gradient Descent(406/499): loss=0.7897318322838811\n",
      "stoch Gradient Descent(407/499): loss=0.7913452714349072\n",
      "stoch Gradient Descent(408/499): loss=0.7915605688961806\n",
      "stoch Gradient Descent(409/499): loss=0.7923503238860793\n",
      "stoch Gradient Descent(410/499): loss=0.7933587436007414\n",
      "stoch Gradient Descent(411/499): loss=0.7884396522741199\n",
      "stoch Gradient Descent(412/499): loss=0.8015670015540838\n",
      "stoch Gradient Descent(413/499): loss=0.7951875531130242\n",
      "stoch Gradient Descent(414/499): loss=0.7929980929738188\n",
      "stoch Gradient Descent(415/499): loss=0.7800351992845177\n",
      "stoch Gradient Descent(416/499): loss=0.792765423548474\n",
      "stoch Gradient Descent(417/499): loss=0.7911807253639327\n",
      "stoch Gradient Descent(418/499): loss=0.7842745302321552\n",
      "stoch Gradient Descent(419/499): loss=0.7930209715718968\n",
      "stoch Gradient Descent(420/499): loss=0.7933255541708302\n",
      "stoch Gradient Descent(421/499): loss=0.7887436341389058\n",
      "stoch Gradient Descent(422/499): loss=0.7920870311924768\n",
      "stoch Gradient Descent(423/499): loss=0.8012910733494973\n",
      "stoch Gradient Descent(424/499): loss=0.7966578500933403\n",
      "stoch Gradient Descent(425/499): loss=0.7853805570478553\n",
      "stoch Gradient Descent(426/499): loss=0.7996351013067129\n",
      "stoch Gradient Descent(427/499): loss=0.8041459399017367\n",
      "stoch Gradient Descent(428/499): loss=0.7943302886789186\n",
      "stoch Gradient Descent(429/499): loss=0.8043823140148288\n",
      "stoch Gradient Descent(430/499): loss=0.7972630307092968\n",
      "stoch Gradient Descent(431/499): loss=0.7889179832382689\n",
      "stoch Gradient Descent(432/499): loss=0.799152657306294\n",
      "stoch Gradient Descent(433/499): loss=0.7976506903083963\n",
      "stoch Gradient Descent(434/499): loss=0.7876313355745062\n",
      "stoch Gradient Descent(435/499): loss=0.7973770155832026\n",
      "stoch Gradient Descent(436/499): loss=0.7970141414125769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoch Gradient Descent(437/499): loss=0.7900002671444093\n",
      "stoch Gradient Descent(438/499): loss=0.7898304897120153\n",
      "stoch Gradient Descent(439/499): loss=0.7900841648543794\n",
      "stoch Gradient Descent(440/499): loss=0.7878673670731702\n",
      "stoch Gradient Descent(441/499): loss=0.7811744261804803\n",
      "stoch Gradient Descent(442/499): loss=0.7933050045201502\n",
      "stoch Gradient Descent(443/499): loss=0.7809315712559937\n",
      "stoch Gradient Descent(444/499): loss=0.7935161965999538\n",
      "stoch Gradient Descent(445/499): loss=0.784346073610442\n",
      "stoch Gradient Descent(446/499): loss=0.7986953618346734\n",
      "stoch Gradient Descent(447/499): loss=0.7823977919892249\n",
      "stoch Gradient Descent(448/499): loss=0.7913061202313116\n",
      "stoch Gradient Descent(449/499): loss=0.7857303057764536\n",
      "stoch Gradient Descent(450/499): loss=0.7955469524389579\n",
      "stoch Gradient Descent(451/499): loss=0.7974847871216173\n",
      "stoch Gradient Descent(452/499): loss=0.7899281461620575\n",
      "stoch Gradient Descent(453/499): loss=0.7913736624724342\n",
      "stoch Gradient Descent(454/499): loss=0.8003804914506317\n",
      "stoch Gradient Descent(455/499): loss=0.8004305735561223\n",
      "stoch Gradient Descent(456/499): loss=0.7950708703421274\n",
      "stoch Gradient Descent(457/499): loss=0.7926398884973898\n",
      "stoch Gradient Descent(458/499): loss=0.7832658630796914\n",
      "stoch Gradient Descent(459/499): loss=0.7880637738865044\n",
      "stoch Gradient Descent(460/499): loss=0.7834940104478817\n",
      "stoch Gradient Descent(461/499): loss=0.7830044095550432\n",
      "stoch Gradient Descent(462/499): loss=0.7865570234455062\n",
      "stoch Gradient Descent(463/499): loss=0.7816663949659802\n",
      "stoch Gradient Descent(464/499): loss=0.7906742929548509\n",
      "stoch Gradient Descent(465/499): loss=0.7883525956141875\n",
      "stoch Gradient Descent(466/499): loss=0.7878130968554644\n",
      "stoch Gradient Descent(467/499): loss=0.7869950890182\n",
      "stoch Gradient Descent(468/499): loss=0.7971264220877816\n",
      "stoch Gradient Descent(469/499): loss=0.7970057165319596\n",
      "stoch Gradient Descent(470/499): loss=0.7928085503920704\n",
      "stoch Gradient Descent(471/499): loss=0.786819338138512\n",
      "stoch Gradient Descent(472/499): loss=0.8026094581655878\n",
      "stoch Gradient Descent(473/499): loss=0.7697222937016269\n",
      "stoch Gradient Descent(474/499): loss=0.7881572295317095\n",
      "stoch Gradient Descent(475/499): loss=0.8003697321614721\n",
      "stoch Gradient Descent(476/499): loss=0.7904680565448534\n",
      "stoch Gradient Descent(477/499): loss=0.7971294947057087\n",
      "stoch Gradient Descent(478/499): loss=0.7808280479599018\n",
      "stoch Gradient Descent(479/499): loss=0.783154277260169\n",
      "stoch Gradient Descent(480/499): loss=0.7898388469931276\n",
      "stoch Gradient Descent(481/499): loss=0.7831162457398357\n",
      "stoch Gradient Descent(482/499): loss=0.7833605608416873\n",
      "stoch Gradient Descent(483/499): loss=0.7876664982715\n",
      "stoch Gradient Descent(484/499): loss=0.793773960713119\n",
      "stoch Gradient Descent(485/499): loss=0.7956167920131606\n",
      "stoch Gradient Descent(486/499): loss=0.786825375333901\n",
      "stoch Gradient Descent(487/499): loss=0.7876344163138183\n",
      "stoch Gradient Descent(488/499): loss=0.7970802473759392\n",
      "stoch Gradient Descent(489/499): loss=0.7957104054584849\n",
      "stoch Gradient Descent(490/499): loss=0.7882019270454577\n",
      "stoch Gradient Descent(491/499): loss=0.7980049270001424\n",
      "stoch Gradient Descent(492/499): loss=0.7880789414336581\n",
      "stoch Gradient Descent(493/499): loss=0.7870682991798136\n",
      "stoch Gradient Descent(494/499): loss=0.7816720683519499\n",
      "stoch Gradient Descent(495/499): loss=0.7898270790610809\n",
      "stoch Gradient Descent(496/499): loss=0.785138563805128\n",
      "stoch Gradient Descent(497/499): loss=0.7735349998039582\n",
      "stoch Gradient Descent(498/499): loss=0.7915646158574784\n",
      "stoch Gradient Descent(499/499): loss=0.7939806557533237\n",
      "SGD: execution time=34.845 seconds\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "from proj1_helpers import batch_iter\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.05\n",
    "batch_size = 10000\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "print(w_initial)\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(\n",
    "    y, tX, w_initial,batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least square stochastic gradient descent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "Gradient Descent(0/49): loss=1.0, w0=53.83324755201073, w1=-13.642099301999938\n",
      "Gradient Descent(1/49): loss=1004083277570.4924, w0=-47840286.291933425, w1=22451160.509754848\n",
      "Gradient Descent(2/49): loss=8.251913580469985e+24, w0=127768620470832.92, w1=-64097460738762.164\n",
      "Gradient Descent(3/49): loss=6.784291354610181e+37, w0=-3.6532296986934156e+20, w1=1.8377016000335243e+20\n",
      "Gradient Descent(4/49): loss=5.5776973393808864e+50, w0=1.047340667076021e+27, w1=-5.269255590689982e+26\n",
      "Gradient Descent(5/49): loss=4.5856975017480705e+63, w0=-3.0030238171911826e+33, w1=1.5108608669042093e+33\n",
      "Gradient Descent(6/49): loss=3.7701259676790215e+76, w0=8.610604774822291e+39, w1=-4.332112251771585e+39\n",
      "Gradient Descent(7/49): loss=3.099604761708956e+89, w0=-2.468930447217248e+46, w1=1.242152536350795e+46\n",
      "Gradient Descent(8/49): loss=2.548336517446234e+102, w0=7.079198370712734e+52, w1=-3.5616411438332573e+52\n",
      "Gradient Descent(9/49): loss=2.09511195955497e+115, w0=-2.029828336870807e+59, w1=1.0212342901692117e+59\n",
      "Gradient Descent(10/49): loss=1.72249390652265e+128, w0=5.820154858303189e+65, w1=-2.928199201716159e+65\n",
      "Gradient Descent(11/49): loss=1.4161463994687587e+141, w0=-1.6688210505604482e+72, w1=8.396066061892703e+71\n",
      "Gradient Descent(12/49): loss=1.1642831461603844e+154, w0=4.785033674541799e+78, w1=-2.407415631913007e+78\n",
      "Gradient Descent(13/49): loss=9.572140598893294e+166, w0=-1.3720193221925407e+85, w1=6.902816130859123e+84\n",
      "Gradient Descent(14/49): loss=7.869724469270929e+179, w0=3.9340099746528856e+91, w1=-1.979254014338415e+91\n",
      "Gradient Descent(15/49): loss=6.470084991167192e+192, w0=-1.1280041199373441e+98, w1=5.675142404216304e+97\n",
      "Gradient Descent(16/49): loss=5.319372991568676e+205, w0=3.2343418110115534e+104, w1=-1.6272414290845857e+104\n",
      "Gradient Descent(17/49): loss=4.373316434337159e+218, w0=-9.273872998831274e+110, w1=4.6658118509271833e+110\n",
      "Gradient Descent(18/49): loss=3.595517115486928e+231, w0=2.6591104287630534e+117, w1=-1.3378346838489658e+117\n",
      "Gradient Descent(19/49): loss=2.9560502931497495e+244, w0=-7.624504102274755e+123, w1=3.835991888428909e+123\n",
      "Gradient Descent(20/49): loss=2.4303133749504235e+257, w0=2.1861846043244886e+130, w1=-1.0998992585360307e+130\n",
      "Gradient Descent(21/49): loss=1.9980793676447663e+270, w0=-6.268477346296484e+136, w1=3.153756353284643e+136\n",
      "Gradient Descent(22/49): loss=1.6427186718211526e+283, w0=1.7973691774841903e+143, w1=-9.042809201564242e+142\n",
      "Gradient Descent(23/49): loss=1.3505592813016118e+296, w0=-5.153621496420955e+149, w1=2.59285718666029e+149\n",
      "Gradient Descent(24/49): loss=inf, w0=1.4777050180391301e+156, w1=-7.434535265050951e+155\n",
      "Gradient Descent(25/49): loss=inf, w0=-4.23704403176375e+162, w1=2.131714576940513e+162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maxime/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in square\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/Maxime/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:70: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=inf, w0=1.2148934941648552e+169, w1=-6.112294683573026e+168\n",
      "Gradient Descent(27/49): loss=inf, w0=-3.483480915230666e+175, w1=1.7525867066338204e+175\n",
      "Gradient Descent(28/49): loss=inf, w0=9.988232997426503e+181, w1=-5.025216098504703e+181\n",
      "Gradient Descent(29/49): loss=inf, w0=-2.863939858969307e+188, w1=1.4408871607370347e+188\n",
      "Gradient Descent(30/49): loss=inf, w0=8.211814359863811e+194, w1=-4.1314756804082574e+194\n",
      "Gradient Descent(31/49): loss=inf, w0=-2.3545848866090442e+201, w1=1.184623734801979e+201\n",
      "Gradient Descent(32/49): loss=inf, w0=6.75133380431142e+207, w1=-3.39668801564262e+207\n",
      "Gradient Descent(33/49): loss=inf, w0=-1.9358192773793115e+214, w1=9.739370516275073e+213\n",
      "Gradient Descent(34/49): loss=inf, w0=5.5506013823228e+220, w1=-2.7925831756244726e+220\n",
      "Gradient Descent(35/49): loss=inf, w0=-1.5915316096631008e+227, w1=8.007212354996864e+226\n",
      "Gradient Descent(36/49): loss=inf, w0=4.56342059190862e+233, w1=-2.295919070832187e+233\n",
      "Gradient Descent(37/49): loss=inf, w0=-1.3084758965650565e+240, w1=6.583120499509952e+239\n",
      "Gradient Descent(38/49): loss=inf, w0=3.751811031679816e+246, w1=-1.8875872438901347e+246\n",
      "Gradient Descent(39/49): loss=inf, w0=-1.075761965076004e+253, w1=5.4123050057521245e+252\n",
      "Gradient Descent(40/49): loss=inf, w0=3.084547157978934e+259, w1=-1.5518776983743225e+259\n",
      "Gradient Descent(41/49): loss=inf, w0=-8.844364718847137e+265, w1=4.449720383740438e+265\n",
      "Gradient Descent(42/49): loss=inf, w0=2.5359569257239533e+272, w1=-1.2758744786536135e+272\n",
      "Gradient Descent(43/49): loss=inf, w0=-7.271384360057844e+278, w1=3.658332535293562e+278\n",
      "Gradient Descent(44/49): loss=inf, w0=2.0849340923485988e+285, w1=-1.0489587465461577e+285\n",
      "Gradient Descent(45/49): loss=inf, w0=-5.978160353227226e+291, w1=3.0076939188563987e+291\n",
      "Gradient Descent(46/49): loss=inf, w0=1.714126184614329e+298, w1=-8.62400236359314e+297\n",
      "Gradient Descent(47/49): loss=inf, w0=-4.914937711890502e+304, w1=2.4727721228873775e+304\n",
      "Gradient Descent(48/49): loss=nan, w0=nan, w1=nan\n",
      "Gradient Descent(49/49): loss=nan, w0=nan, w1=nan\n",
      "SGD: execution time=4.619 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.5\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "print(w_initial)\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_GD(\n",
    "    y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plots'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b688452e5795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Time Visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradient_descent_visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntSlider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plots'"
     ]
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
