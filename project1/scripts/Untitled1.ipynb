{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    return compute_gradient(y, tx, w)\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"Generate a minibatch iterator for a dataset.\n",
    "    Input:\n",
    "    - y, tx: two iterables\n",
    "    - shuffle: data can be randomly shuffled\n",
    "    Output:\n",
    "    - an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    #1 / (1 +np.exp(-t))\n",
    "    return np.exp(-np.logaddexp(0, -t))\n",
    "\n",
    "\n",
    "def compute_logistic_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    #eps = 1e-12\n",
    "    eps = np.finfo(float).eps\n",
    "    sigma = sigmoid(tx.dot(w))\n",
    "    loss = -(y.T.dot(np.log(sigma+eps)) + (1-y).T.dot(np.log(1-sigma+eps))).squeeze()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_logistic_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    sigma = sigmoid(tx.dot(w))\n",
    "    gradient = tx.T.dot(sigma - y)\n",
    "    return gradient\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Linear regression using gradient descent\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Linear regression using stochastic gradient descent\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, err = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"Least squares regression using normal equations\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    # calculate loss\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations\"\"\"\n",
    "    lambda_prime = lambda_ * 2 * tx.shape[0]\n",
    "    lambda_I = lambda_prime * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + lambda_I\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    # calculate loss\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Logistic regression using gradient descent or SGD\"\"\"\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        loss = compute_logistic_loss(y, tx, w)\n",
    "        grad = compute_logistic_gradient(y, tx, w)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Regularized logistic regression using gradient descent or SGD\"\"\"\n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute regularized loss, gradient\n",
    "        loss = compute_logistic_loss(y, tx, w) + lambda_*w.T.dot(w).squeeze()\n",
    "        grad = compute_logistic_gradient(y, tx, w) + 2*lambda_*w\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "def standardization(x, mean, std):\n",
    "    return (x - mean) / (std + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y):\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    return np.count_nonzero(arr==0) / len(y)\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, logistic, regression_method, **kwargs):\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "\n",
    "    if logistic == True:\n",
    "        w_initial = np.zeros(x_train.shape[1])\n",
    "        kwargs = kwargs\n",
    "        kwargs['initial_w'] = w_initial\n",
    "\n",
    "    w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "\n",
    "    loss_test = compute_loss(y_test, x_test, w)\n",
    "    \n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "    \n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    \n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
