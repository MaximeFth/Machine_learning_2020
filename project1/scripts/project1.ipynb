{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import costs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_std = standardize(tX)\n",
    "print(tX_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import *\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    return gradient_descent(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    return stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (30,) and (250000,) not aligned: 30 (dim 0) != 250000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ec1e77155e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Start gradient descent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgradient_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_ws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-15fdf6120b56>\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradient_descent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EPFL/MA2/ml/Projet1/Machine_learning_2020/project1/scripts/gradient_descent.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# ***************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# compute gradient computes the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# compute loss. here MSE is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EPFL/MA2/ml/Projet1/Machine_learning_2020/project1/scripts/gradient_descent.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\"Compute the gradient.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# ***************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# ***************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (30,) and (250000,) not aligned: 30 (dim 0) != 250000 (dim 0)"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX_std, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72343805,  0.52317289,  0.62968263, ...,  0.40687538,\n",
       "         0.3983056 ,  0.66583024],\n",
       "       [ 0.77526501,  0.56264922,  0.64215782, ..., -1.90048197,\n",
       "        -1.90048197,  0.51064925],\n",
       "       [-1.90048197,  0.77811391,  0.69456379, ..., -1.90048197,\n",
       "        -1.90048197,  0.50609331],\n",
       "       ...,\n",
       "       [ 0.64728354,  0.54363654,  0.57896063, ..., -1.90048197,\n",
       "        -1.90048197,  0.50088224],\n",
       "       [ 0.62304826,  0.44867927,  0.56275072, ..., -1.90048197,\n",
       "        -1.90048197,  0.40401494],\n",
       "       [-1.90048197,  0.57184875,  0.56740816, ..., -1.90048197,\n",
       "        -1.90048197,  0.40401494]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize(tX)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "stoch Gradient Descent(0/499): loss=1.0\n",
      "stoch Gradient Descent(1/499): loss=0.8748329763372845\n",
      "stoch Gradient Descent(2/499): loss=0.8750850686707375\n",
      "stoch Gradient Descent(3/499): loss=0.8686686036034618\n",
      "stoch Gradient Descent(4/499): loss=0.8608679148802753\n",
      "stoch Gradient Descent(5/499): loss=0.8607693417990282\n",
      "stoch Gradient Descent(6/499): loss=0.8645060672515698\n",
      "stoch Gradient Descent(7/499): loss=0.8604048066595569\n",
      "stoch Gradient Descent(8/499): loss=0.8532792426369813\n",
      "stoch Gradient Descent(9/499): loss=0.8575215281097782\n",
      "stoch Gradient Descent(10/499): loss=0.8485336421575519\n",
      "stoch Gradient Descent(11/499): loss=0.8520513621131535\n",
      "stoch Gradient Descent(12/499): loss=0.8395967875366114\n",
      "stoch Gradient Descent(13/499): loss=0.8488755017591948\n",
      "stoch Gradient Descent(14/499): loss=0.8539659247005784\n",
      "stoch Gradient Descent(15/499): loss=0.8450843544715863\n",
      "stoch Gradient Descent(16/499): loss=0.8462207705527477\n",
      "stoch Gradient Descent(17/499): loss=0.8476228115966231\n",
      "stoch Gradient Descent(18/499): loss=0.8545183168318014\n",
      "stoch Gradient Descent(19/499): loss=0.8384895718806944\n",
      "stoch Gradient Descent(20/499): loss=0.8280125573851861\n",
      "stoch Gradient Descent(21/499): loss=0.8309526921368624\n",
      "stoch Gradient Descent(22/499): loss=0.8419506838855163\n",
      "stoch Gradient Descent(23/499): loss=0.8326421976789383\n",
      "stoch Gradient Descent(24/499): loss=0.826398062865214\n",
      "stoch Gradient Descent(25/499): loss=0.8272533923201785\n",
      "stoch Gradient Descent(26/499): loss=0.8283559278789917\n",
      "stoch Gradient Descent(27/499): loss=0.8285870907799991\n",
      "stoch Gradient Descent(28/499): loss=0.8292085401824876\n",
      "stoch Gradient Descent(29/499): loss=0.83136801480139\n",
      "stoch Gradient Descent(30/499): loss=0.8311398324301079\n",
      "stoch Gradient Descent(31/499): loss=0.8299782883395576\n",
      "stoch Gradient Descent(32/499): loss=0.8441655929547974\n",
      "stoch Gradient Descent(33/499): loss=0.8269810199798411\n",
      "stoch Gradient Descent(34/499): loss=0.8215554541810676\n",
      "stoch Gradient Descent(35/499): loss=0.830915250412604\n",
      "stoch Gradient Descent(36/499): loss=0.8269027444048128\n",
      "stoch Gradient Descent(37/499): loss=0.8301357418145914\n",
      "stoch Gradient Descent(38/499): loss=0.8298018176548067\n",
      "stoch Gradient Descent(39/499): loss=0.8152716295465965\n",
      "stoch Gradient Descent(40/499): loss=0.8226448822530874\n",
      "stoch Gradient Descent(41/499): loss=0.8265864492038905\n",
      "stoch Gradient Descent(42/499): loss=0.8167872147341135\n",
      "stoch Gradient Descent(43/499): loss=0.8262944599720968\n",
      "stoch Gradient Descent(44/499): loss=0.8134207411420309\n",
      "stoch Gradient Descent(45/499): loss=0.8268086245581034\n",
      "stoch Gradient Descent(46/499): loss=0.8177432635543868\n",
      "stoch Gradient Descent(47/499): loss=0.8176370648626413\n",
      "stoch Gradient Descent(48/499): loss=0.8205824135426566\n",
      "stoch Gradient Descent(49/499): loss=0.8182878905047771\n",
      "stoch Gradient Descent(50/499): loss=0.8124921793341616\n",
      "stoch Gradient Descent(51/499): loss=0.8175034128986416\n",
      "stoch Gradient Descent(52/499): loss=0.8247551992019518\n",
      "stoch Gradient Descent(53/499): loss=0.8235190454194379\n",
      "stoch Gradient Descent(54/499): loss=0.8173225461735029\n",
      "stoch Gradient Descent(55/499): loss=0.8068137046679084\n",
      "stoch Gradient Descent(56/499): loss=0.8283529519416883\n",
      "stoch Gradient Descent(57/499): loss=0.8212965827868375\n",
      "stoch Gradient Descent(58/499): loss=0.8088567415204804\n",
      "stoch Gradient Descent(59/499): loss=0.8120374359709783\n",
      "stoch Gradient Descent(60/499): loss=0.8302306181698186\n",
      "stoch Gradient Descent(61/499): loss=0.8174162754173095\n",
      "stoch Gradient Descent(62/499): loss=0.8171519592760502\n",
      "stoch Gradient Descent(63/499): loss=0.8169164198959273\n",
      "stoch Gradient Descent(64/499): loss=0.8172210182628371\n",
      "stoch Gradient Descent(65/499): loss=0.8210956928218253\n",
      "stoch Gradient Descent(66/499): loss=0.8142728481675101\n",
      "stoch Gradient Descent(67/499): loss=0.8128627604925674\n",
      "stoch Gradient Descent(68/499): loss=0.8101600186866474\n",
      "stoch Gradient Descent(69/499): loss=0.8172662107034208\n",
      "stoch Gradient Descent(70/499): loss=0.819732048404618\n",
      "stoch Gradient Descent(71/499): loss=0.816190313695673\n",
      "stoch Gradient Descent(72/499): loss=0.8130867759667432\n",
      "stoch Gradient Descent(73/499): loss=0.8093407243814361\n",
      "stoch Gradient Descent(74/499): loss=0.8155044422474084\n",
      "stoch Gradient Descent(75/499): loss=0.8139136457540093\n",
      "stoch Gradient Descent(76/499): loss=0.8119952755892177\n",
      "stoch Gradient Descent(77/499): loss=0.805793181392286\n",
      "stoch Gradient Descent(78/499): loss=0.8073037407328633\n",
      "stoch Gradient Descent(79/499): loss=0.8116987658210634\n",
      "stoch Gradient Descent(80/499): loss=0.8102414269483926\n",
      "stoch Gradient Descent(81/499): loss=0.8076688827192513\n",
      "stoch Gradient Descent(82/499): loss=0.8124107226090187\n",
      "stoch Gradient Descent(83/499): loss=0.813847420176431\n",
      "stoch Gradient Descent(84/499): loss=0.8049369572240839\n",
      "stoch Gradient Descent(85/499): loss=0.8125196813904485\n",
      "stoch Gradient Descent(86/499): loss=0.8203041361353276\n",
      "stoch Gradient Descent(87/499): loss=0.8162909977108882\n",
      "stoch Gradient Descent(88/499): loss=0.8070181440542061\n",
      "stoch Gradient Descent(89/499): loss=0.803652492996675\n",
      "stoch Gradient Descent(90/499): loss=0.8034699538872331\n",
      "stoch Gradient Descent(91/499): loss=0.8136984326289758\n",
      "stoch Gradient Descent(92/499): loss=0.8101473058612024\n",
      "stoch Gradient Descent(93/499): loss=0.8157967368212102\n",
      "stoch Gradient Descent(94/499): loss=0.8139560341693078\n",
      "stoch Gradient Descent(95/499): loss=0.8076756678795411\n",
      "stoch Gradient Descent(96/499): loss=0.8132080922596768\n",
      "stoch Gradient Descent(97/499): loss=0.8034790199164001\n",
      "stoch Gradient Descent(98/499): loss=0.8127694357298051\n",
      "stoch Gradient Descent(99/499): loss=0.7947793381254558\n",
      "stoch Gradient Descent(100/499): loss=0.7994074287230506\n",
      "stoch Gradient Descent(101/499): loss=0.7975563391198208\n",
      "stoch Gradient Descent(102/499): loss=0.8162062715566218\n",
      "stoch Gradient Descent(103/499): loss=0.8060928222510092\n",
      "stoch Gradient Descent(104/499): loss=0.8084444324763664\n",
      "stoch Gradient Descent(105/499): loss=0.8121057306308316\n",
      "stoch Gradient Descent(106/499): loss=0.8006361621656864\n",
      "stoch Gradient Descent(107/499): loss=0.812047258018054\n",
      "stoch Gradient Descent(108/499): loss=0.8122713565817882\n",
      "stoch Gradient Descent(109/499): loss=0.8164381967505542\n",
      "stoch Gradient Descent(110/499): loss=0.8050034187880267\n",
      "stoch Gradient Descent(111/499): loss=0.7997584969802072\n",
      "stoch Gradient Descent(112/499): loss=0.8129346055687175\n",
      "stoch Gradient Descent(113/499): loss=0.8072556424469463\n",
      "stoch Gradient Descent(114/499): loss=0.8085036341352787\n",
      "stoch Gradient Descent(115/499): loss=0.8057350381202093\n",
      "stoch Gradient Descent(116/499): loss=0.797754134382965\n",
      "stoch Gradient Descent(117/499): loss=0.8060480147714492\n",
      "stoch Gradient Descent(118/499): loss=0.7986947961456301\n",
      "stoch Gradient Descent(119/499): loss=0.8034182404620287\n",
      "stoch Gradient Descent(120/499): loss=0.8140157164648101\n",
      "stoch Gradient Descent(121/499): loss=0.8017778762557441\n",
      "stoch Gradient Descent(122/499): loss=0.8082452742304996\n",
      "stoch Gradient Descent(123/499): loss=0.7974371291014991\n",
      "stoch Gradient Descent(124/499): loss=0.8150485735277274\n",
      "stoch Gradient Descent(125/499): loss=0.8135310063332063\n",
      "stoch Gradient Descent(126/499): loss=0.8054480578367724\n",
      "stoch Gradient Descent(127/499): loss=0.8039080891040716\n",
      "stoch Gradient Descent(128/499): loss=0.8072973443629894\n",
      "stoch Gradient Descent(129/499): loss=0.8021024933243235\n",
      "stoch Gradient Descent(130/499): loss=0.8090263079398893\n",
      "stoch Gradient Descent(131/499): loss=0.8007994361994192\n",
      "stoch Gradient Descent(132/499): loss=0.8046224497172967\n",
      "stoch Gradient Descent(133/499): loss=0.8134182892487136\n",
      "stoch Gradient Descent(134/499): loss=0.8165573648604779\n",
      "stoch Gradient Descent(135/499): loss=0.8099077136369884\n",
      "stoch Gradient Descent(136/499): loss=0.8123946234490766\n",
      "stoch Gradient Descent(137/499): loss=0.7974611123159339\n",
      "stoch Gradient Descent(138/499): loss=0.7960119439535032\n",
      "stoch Gradient Descent(139/499): loss=0.8006114241312388\n",
      "stoch Gradient Descent(140/499): loss=0.8108294887795386\n",
      "stoch Gradient Descent(141/499): loss=0.8092958217514331\n",
      "stoch Gradient Descent(142/499): loss=0.8114906739159701\n",
      "stoch Gradient Descent(143/499): loss=0.798911740938471\n",
      "stoch Gradient Descent(144/499): loss=0.7939624111078487\n",
      "stoch Gradient Descent(145/499): loss=0.8051509554959942\n",
      "stoch Gradient Descent(146/499): loss=0.8070288898848558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoch Gradient Descent(147/499): loss=0.8061374915319588\n",
      "stoch Gradient Descent(148/499): loss=0.8057495594774132\n",
      "stoch Gradient Descent(149/499): loss=0.8087177981046925\n",
      "stoch Gradient Descent(150/499): loss=0.7982180081773258\n",
      "stoch Gradient Descent(151/499): loss=0.7984030151537261\n",
      "stoch Gradient Descent(152/499): loss=0.8011568911031891\n",
      "stoch Gradient Descent(153/499): loss=0.8063763864696846\n",
      "stoch Gradient Descent(154/499): loss=0.8061577064439022\n",
      "stoch Gradient Descent(155/499): loss=0.7963395454379539\n",
      "stoch Gradient Descent(156/499): loss=0.7966018660329781\n",
      "stoch Gradient Descent(157/499): loss=0.806803437401804\n",
      "stoch Gradient Descent(158/499): loss=0.8118941246472636\n",
      "stoch Gradient Descent(159/499): loss=0.7947371277242355\n",
      "stoch Gradient Descent(160/499): loss=0.8046814947530171\n",
      "stoch Gradient Descent(161/499): loss=0.8048720667967942\n",
      "stoch Gradient Descent(162/499): loss=0.8090859474777933\n",
      "stoch Gradient Descent(163/499): loss=0.794012111703444\n",
      "stoch Gradient Descent(164/499): loss=0.7984166077420607\n",
      "stoch Gradient Descent(165/499): loss=0.8049552445357419\n",
      "stoch Gradient Descent(166/499): loss=0.8141095205172129\n",
      "stoch Gradient Descent(167/499): loss=0.8040607790115158\n",
      "stoch Gradient Descent(168/499): loss=0.8075327294990677\n",
      "stoch Gradient Descent(169/499): loss=0.8053624530957109\n",
      "stoch Gradient Descent(170/499): loss=0.7978249135522635\n",
      "stoch Gradient Descent(171/499): loss=0.8006540481705973\n",
      "stoch Gradient Descent(172/499): loss=0.8041065440184659\n",
      "stoch Gradient Descent(173/499): loss=0.8022478961613647\n",
      "stoch Gradient Descent(174/499): loss=0.7990728813323515\n",
      "stoch Gradient Descent(175/499): loss=0.7927412048938368\n",
      "stoch Gradient Descent(176/499): loss=0.8081927568722964\n",
      "stoch Gradient Descent(177/499): loss=0.8024615682448294\n",
      "stoch Gradient Descent(178/499): loss=0.8061361595470394\n",
      "stoch Gradient Descent(179/499): loss=0.7980668069450607\n",
      "stoch Gradient Descent(180/499): loss=0.7976926581510366\n",
      "stoch Gradient Descent(181/499): loss=0.8046686063987422\n",
      "stoch Gradient Descent(182/499): loss=0.8070033246694145\n",
      "stoch Gradient Descent(183/499): loss=0.7989850559334852\n",
      "stoch Gradient Descent(184/499): loss=0.7940188722671918\n",
      "stoch Gradient Descent(185/499): loss=0.7963064760395506\n",
      "stoch Gradient Descent(186/499): loss=0.7964619588127388\n",
      "stoch Gradient Descent(187/499): loss=0.8055241487834197\n",
      "stoch Gradient Descent(188/499): loss=0.7971576267645887\n",
      "stoch Gradient Descent(189/499): loss=0.8041122300233797\n",
      "stoch Gradient Descent(190/499): loss=0.7994889775052361\n",
      "stoch Gradient Descent(191/499): loss=0.8054781857416554\n",
      "stoch Gradient Descent(192/499): loss=0.7973050264414882\n",
      "stoch Gradient Descent(193/499): loss=0.8031518189134796\n",
      "stoch Gradient Descent(194/499): loss=0.8040687559001994\n",
      "stoch Gradient Descent(195/499): loss=0.7952841199510096\n",
      "stoch Gradient Descent(196/499): loss=0.8065595929555579\n",
      "stoch Gradient Descent(197/499): loss=0.8004110400959586\n",
      "stoch Gradient Descent(198/499): loss=0.8091933830827313\n",
      "stoch Gradient Descent(199/499): loss=0.7832543503997593\n",
      "stoch Gradient Descent(200/499): loss=0.7886105451409432\n",
      "stoch Gradient Descent(201/499): loss=0.7967149260803464\n",
      "stoch Gradient Descent(202/499): loss=0.807769502923226\n",
      "stoch Gradient Descent(203/499): loss=0.8047844842728041\n",
      "stoch Gradient Descent(204/499): loss=0.7992932700106303\n",
      "stoch Gradient Descent(205/499): loss=0.7988988682045352\n",
      "stoch Gradient Descent(206/499): loss=0.7987385422547172\n",
      "stoch Gradient Descent(207/499): loss=0.7954583828883252\n",
      "stoch Gradient Descent(208/499): loss=0.798889410078403\n",
      "stoch Gradient Descent(209/499): loss=0.8134229126296606\n",
      "stoch Gradient Descent(210/499): loss=0.7959152988896214\n",
      "stoch Gradient Descent(211/499): loss=0.8143007038071751\n",
      "stoch Gradient Descent(212/499): loss=0.8050257083052922\n",
      "stoch Gradient Descent(213/499): loss=0.8024281728709886\n",
      "stoch Gradient Descent(214/499): loss=0.7998853214769422\n",
      "stoch Gradient Descent(215/499): loss=0.8024637898111783\n",
      "stoch Gradient Descent(216/499): loss=0.7983065725224476\n",
      "stoch Gradient Descent(217/499): loss=0.8017866564224145\n",
      "stoch Gradient Descent(218/499): loss=0.7963898439651448\n",
      "stoch Gradient Descent(219/499): loss=0.7972332901912079\n",
      "stoch Gradient Descent(220/499): loss=0.8051414404294779\n",
      "stoch Gradient Descent(221/499): loss=0.7844311649162471\n",
      "stoch Gradient Descent(222/499): loss=0.8014258066452503\n",
      "stoch Gradient Descent(223/499): loss=0.7988540127536496\n",
      "stoch Gradient Descent(224/499): loss=0.7955495154801284\n",
      "stoch Gradient Descent(225/499): loss=0.7876097841450785\n",
      "stoch Gradient Descent(226/499): loss=0.7946392038616891\n",
      "stoch Gradient Descent(227/499): loss=0.8078398067676713\n",
      "stoch Gradient Descent(228/499): loss=0.7996578385363607\n",
      "stoch Gradient Descent(229/499): loss=0.7925157735274487\n",
      "stoch Gradient Descent(230/499): loss=0.7937013863599476\n",
      "stoch Gradient Descent(231/499): loss=0.7989113774631396\n",
      "stoch Gradient Descent(232/499): loss=0.7936324937760104\n",
      "stoch Gradient Descent(233/499): loss=0.7912906994763876\n",
      "stoch Gradient Descent(234/499): loss=0.8020429910014937\n",
      "stoch Gradient Descent(235/499): loss=0.790742880657299\n",
      "stoch Gradient Descent(236/499): loss=0.8007950333380847\n",
      "stoch Gradient Descent(237/499): loss=0.7999502859183006\n",
      "stoch Gradient Descent(238/499): loss=0.7935204140886238\n",
      "stoch Gradient Descent(239/499): loss=0.7869645855643457\n",
      "stoch Gradient Descent(240/499): loss=0.7882182576686202\n",
      "stoch Gradient Descent(241/499): loss=0.8050800094549992\n",
      "stoch Gradient Descent(242/499): loss=0.7972644211814238\n",
      "stoch Gradient Descent(243/499): loss=0.8050998370840411\n",
      "stoch Gradient Descent(244/499): loss=0.7984826218557143\n",
      "stoch Gradient Descent(245/499): loss=0.807354440783133\n",
      "stoch Gradient Descent(246/499): loss=0.7950707576604744\n",
      "stoch Gradient Descent(247/499): loss=0.7968117879304634\n",
      "stoch Gradient Descent(248/499): loss=0.7944886425303356\n",
      "stoch Gradient Descent(249/499): loss=0.8043036148480021\n",
      "stoch Gradient Descent(250/499): loss=0.8049953532846604\n",
      "stoch Gradient Descent(251/499): loss=0.8043509533508009\n",
      "stoch Gradient Descent(252/499): loss=0.7864501553773318\n",
      "stoch Gradient Descent(253/499): loss=0.7844784951686453\n",
      "stoch Gradient Descent(254/499): loss=0.8030511220554953\n",
      "stoch Gradient Descent(255/499): loss=0.7989798245900549\n",
      "stoch Gradient Descent(256/499): loss=0.7969886890066407\n",
      "stoch Gradient Descent(257/499): loss=0.7833993321760278\n",
      "stoch Gradient Descent(258/499): loss=0.7945878762723193\n",
      "stoch Gradient Descent(259/499): loss=0.799852002711147\n",
      "stoch Gradient Descent(260/499): loss=0.8047834156797622\n",
      "stoch Gradient Descent(261/499): loss=0.7975525199413264\n",
      "stoch Gradient Descent(262/499): loss=0.8027207193688922\n",
      "stoch Gradient Descent(263/499): loss=0.7978030852855589\n",
      "stoch Gradient Descent(264/499): loss=0.7991365838040592\n",
      "stoch Gradient Descent(265/499): loss=0.7877925729308322\n",
      "stoch Gradient Descent(266/499): loss=0.808876313810469\n",
      "stoch Gradient Descent(267/499): loss=0.8047217694452813\n",
      "stoch Gradient Descent(268/499): loss=0.795173410367992\n",
      "stoch Gradient Descent(269/499): loss=0.8007791516718358\n",
      "stoch Gradient Descent(270/499): loss=0.7860100853221915\n",
      "stoch Gradient Descent(271/499): loss=0.7911787673180841\n",
      "stoch Gradient Descent(272/499): loss=0.7970902502421339\n",
      "stoch Gradient Descent(273/499): loss=0.7915528880160374\n",
      "stoch Gradient Descent(274/499): loss=0.80374020085824\n",
      "stoch Gradient Descent(275/499): loss=0.8083562599111223\n",
      "stoch Gradient Descent(276/499): loss=0.8025746843561292\n",
      "stoch Gradient Descent(277/499): loss=0.7946853431573269\n",
      "stoch Gradient Descent(278/499): loss=0.7964105586735357\n",
      "stoch Gradient Descent(279/499): loss=0.7934718193772132\n",
      "stoch Gradient Descent(280/499): loss=0.8005653889185893\n",
      "stoch Gradient Descent(281/499): loss=0.8123196386010054\n",
      "stoch Gradient Descent(282/499): loss=0.7899601166732291\n",
      "stoch Gradient Descent(283/499): loss=0.8006296968100907\n",
      "stoch Gradient Descent(284/499): loss=0.7938026442409507\n",
      "stoch Gradient Descent(285/499): loss=0.8025290305278502\n",
      "stoch Gradient Descent(286/499): loss=0.7898872445659832\n",
      "stoch Gradient Descent(287/499): loss=0.7849853471051074\n",
      "stoch Gradient Descent(288/499): loss=0.793280373176811\n",
      "stoch Gradient Descent(289/499): loss=0.7866883486993519\n",
      "stoch Gradient Descent(290/499): loss=0.7941066715205799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoch Gradient Descent(291/499): loss=0.7918754921128127\n",
      "stoch Gradient Descent(292/499): loss=0.7947102942921181\n",
      "stoch Gradient Descent(293/499): loss=0.7970729833823954\n",
      "stoch Gradient Descent(294/499): loss=0.7925503747669687\n",
      "stoch Gradient Descent(295/499): loss=0.7964708537209197\n",
      "stoch Gradient Descent(296/499): loss=0.7882957711192141\n",
      "stoch Gradient Descent(297/499): loss=0.7973246855470593\n",
      "stoch Gradient Descent(298/499): loss=0.7932278908118691\n",
      "stoch Gradient Descent(299/499): loss=0.7949484684293783\n",
      "stoch Gradient Descent(300/499): loss=0.7970251341977042\n",
      "stoch Gradient Descent(301/499): loss=0.7916818818127959\n",
      "stoch Gradient Descent(302/499): loss=0.7938200634785672\n",
      "stoch Gradient Descent(303/499): loss=0.8042259855063839\n",
      "stoch Gradient Descent(304/499): loss=0.7922766011903123\n",
      "stoch Gradient Descent(305/499): loss=0.8007922907847418\n",
      "stoch Gradient Descent(306/499): loss=0.8058251280523876\n",
      "stoch Gradient Descent(307/499): loss=0.7963160078009652\n",
      "stoch Gradient Descent(308/499): loss=0.7948897199344765\n",
      "stoch Gradient Descent(309/499): loss=0.8093255803823699\n",
      "stoch Gradient Descent(310/499): loss=0.7990407091422167\n",
      "stoch Gradient Descent(311/499): loss=0.7917670746304534\n",
      "stoch Gradient Descent(312/499): loss=0.7887449634818339\n",
      "stoch Gradient Descent(313/499): loss=0.802059233777038\n",
      "stoch Gradient Descent(314/499): loss=0.79215321908418\n",
      "stoch Gradient Descent(315/499): loss=0.7979697986811037\n",
      "stoch Gradient Descent(316/499): loss=0.7890847509938137\n",
      "stoch Gradient Descent(317/499): loss=0.7988311092537141\n",
      "stoch Gradient Descent(318/499): loss=0.7917679688173069\n",
      "stoch Gradient Descent(319/499): loss=0.78751678517595\n",
      "stoch Gradient Descent(320/499): loss=0.795385808785938\n",
      "stoch Gradient Descent(321/499): loss=0.7970624356805928\n",
      "stoch Gradient Descent(322/499): loss=0.798187434694406\n",
      "stoch Gradient Descent(323/499): loss=0.7966630908669476\n",
      "stoch Gradient Descent(324/499): loss=0.7915020145221715\n",
      "stoch Gradient Descent(325/499): loss=0.7951738546928934\n",
      "stoch Gradient Descent(326/499): loss=0.794854006671193\n",
      "stoch Gradient Descent(327/499): loss=0.8103603575644244\n",
      "stoch Gradient Descent(328/499): loss=0.7911062314958084\n",
      "stoch Gradient Descent(329/499): loss=0.7900507185236232\n",
      "stoch Gradient Descent(330/499): loss=0.7930879926283302\n",
      "stoch Gradient Descent(331/499): loss=0.7951536854410398\n",
      "stoch Gradient Descent(332/499): loss=0.7946255760104088\n",
      "stoch Gradient Descent(333/499): loss=0.7882905926612904\n",
      "stoch Gradient Descent(334/499): loss=0.7811742831010079\n",
      "stoch Gradient Descent(335/499): loss=0.7963253897717217\n",
      "stoch Gradient Descent(336/499): loss=0.7931248575151846\n",
      "stoch Gradient Descent(337/499): loss=0.8038440037320286\n",
      "stoch Gradient Descent(338/499): loss=0.7915459415869154\n",
      "stoch Gradient Descent(339/499): loss=0.7869876822847386\n",
      "stoch Gradient Descent(340/499): loss=0.7911776961125053\n",
      "stoch Gradient Descent(341/499): loss=0.7842134215144719\n",
      "stoch Gradient Descent(342/499): loss=0.7917115533601966\n",
      "stoch Gradient Descent(343/499): loss=0.7970239506994774\n",
      "stoch Gradient Descent(344/499): loss=0.8035069705337251\n",
      "stoch Gradient Descent(345/499): loss=0.7914985657671103\n",
      "stoch Gradient Descent(346/499): loss=0.8010106424866849\n",
      "stoch Gradient Descent(347/499): loss=0.7898444954062468\n",
      "stoch Gradient Descent(348/499): loss=0.7906882926237395\n",
      "stoch Gradient Descent(349/499): loss=0.7952512841255241\n",
      "stoch Gradient Descent(350/499): loss=0.7881076589017266\n",
      "stoch Gradient Descent(351/499): loss=0.7887997527827595\n",
      "stoch Gradient Descent(352/499): loss=0.7928956406436537\n",
      "stoch Gradient Descent(353/499): loss=0.7865152609923676\n",
      "stoch Gradient Descent(354/499): loss=0.7920223300184238\n",
      "stoch Gradient Descent(355/499): loss=0.7945315213459978\n",
      "stoch Gradient Descent(356/499): loss=0.7939126694742579\n",
      "stoch Gradient Descent(357/499): loss=0.7930700983593798\n",
      "stoch Gradient Descent(358/499): loss=0.7916091867118206\n",
      "stoch Gradient Descent(359/499): loss=0.794107969583114\n",
      "stoch Gradient Descent(360/499): loss=0.7855526897690179\n",
      "stoch Gradient Descent(361/499): loss=0.7998959770721018\n",
      "stoch Gradient Descent(362/499): loss=0.7902514217387845\n",
      "stoch Gradient Descent(363/499): loss=0.7931505849008856\n",
      "stoch Gradient Descent(364/499): loss=0.8008326099950688\n",
      "stoch Gradient Descent(365/499): loss=0.7951002375073107\n",
      "stoch Gradient Descent(366/499): loss=0.7934603818453234\n",
      "stoch Gradient Descent(367/499): loss=0.791174662153245\n",
      "stoch Gradient Descent(368/499): loss=0.7896380610702688\n",
      "stoch Gradient Descent(369/499): loss=0.7894414780599033\n",
      "stoch Gradient Descent(370/499): loss=0.7934284784445849\n",
      "stoch Gradient Descent(371/499): loss=0.7905550236559583\n",
      "stoch Gradient Descent(372/499): loss=0.7940170727560334\n",
      "stoch Gradient Descent(373/499): loss=0.7838990328718317\n",
      "stoch Gradient Descent(374/499): loss=0.7991486337395725\n",
      "stoch Gradient Descent(375/499): loss=0.7901769577373644\n",
      "stoch Gradient Descent(376/499): loss=0.7877230094391099\n",
      "stoch Gradient Descent(377/499): loss=0.7921274435652773\n",
      "stoch Gradient Descent(378/499): loss=0.7918425444321097\n",
      "stoch Gradient Descent(379/499): loss=0.7878931135025014\n",
      "stoch Gradient Descent(380/499): loss=0.7866482439699396\n",
      "stoch Gradient Descent(381/499): loss=0.7908621219752988\n",
      "stoch Gradient Descent(382/499): loss=0.7904727638093892\n",
      "stoch Gradient Descent(383/499): loss=0.8023005751698221\n",
      "stoch Gradient Descent(384/499): loss=0.7941310238021099\n",
      "stoch Gradient Descent(385/499): loss=0.7922483481427355\n",
      "stoch Gradient Descent(386/499): loss=0.795531158372592\n",
      "stoch Gradient Descent(387/499): loss=0.7993675005259134\n",
      "stoch Gradient Descent(388/499): loss=0.7932106519244825\n",
      "stoch Gradient Descent(389/499): loss=0.7917420486023222\n",
      "stoch Gradient Descent(390/499): loss=0.8013625979607467\n",
      "stoch Gradient Descent(391/499): loss=0.785259693387838\n",
      "stoch Gradient Descent(392/499): loss=0.7931155299257928\n",
      "stoch Gradient Descent(393/499): loss=0.7895573342497133\n",
      "stoch Gradient Descent(394/499): loss=0.7959794886668112\n",
      "stoch Gradient Descent(395/499): loss=0.7918711579004155\n",
      "stoch Gradient Descent(396/499): loss=0.7932725495559022\n",
      "stoch Gradient Descent(397/499): loss=0.7933191660820801\n",
      "stoch Gradient Descent(398/499): loss=0.7932905672860855\n",
      "stoch Gradient Descent(399/499): loss=0.8002446106012697\n",
      "stoch Gradient Descent(400/499): loss=0.7992318195773407\n",
      "stoch Gradient Descent(401/499): loss=0.7881451920206388\n",
      "stoch Gradient Descent(402/499): loss=0.795749740427392\n",
      "stoch Gradient Descent(403/499): loss=0.7932567646163505\n",
      "stoch Gradient Descent(404/499): loss=0.7840688658083841\n",
      "stoch Gradient Descent(405/499): loss=0.7833567471317808\n",
      "stoch Gradient Descent(406/499): loss=0.7897318322838811\n",
      "stoch Gradient Descent(407/499): loss=0.7913452714349072\n",
      "stoch Gradient Descent(408/499): loss=0.7915605688961806\n",
      "stoch Gradient Descent(409/499): loss=0.7923503238860793\n",
      "stoch Gradient Descent(410/499): loss=0.7933587436007414\n",
      "stoch Gradient Descent(411/499): loss=0.7884396522741199\n",
      "stoch Gradient Descent(412/499): loss=0.8015670015540838\n",
      "stoch Gradient Descent(413/499): loss=0.7951875531130242\n",
      "stoch Gradient Descent(414/499): loss=0.7929980929738188\n",
      "stoch Gradient Descent(415/499): loss=0.7800351992845177\n",
      "stoch Gradient Descent(416/499): loss=0.792765423548474\n",
      "stoch Gradient Descent(417/499): loss=0.7911807253639327\n",
      "stoch Gradient Descent(418/499): loss=0.7842745302321552\n",
      "stoch Gradient Descent(419/499): loss=0.7930209715718968\n",
      "stoch Gradient Descent(420/499): loss=0.7933255541708302\n",
      "stoch Gradient Descent(421/499): loss=0.7887436341389058\n",
      "stoch Gradient Descent(422/499): loss=0.7920870311924768\n",
      "stoch Gradient Descent(423/499): loss=0.8012910733494973\n",
      "stoch Gradient Descent(424/499): loss=0.7966578500933403\n",
      "stoch Gradient Descent(425/499): loss=0.7853805570478553\n",
      "stoch Gradient Descent(426/499): loss=0.7996351013067129\n",
      "stoch Gradient Descent(427/499): loss=0.8041459399017367\n",
      "stoch Gradient Descent(428/499): loss=0.7943302886789186\n",
      "stoch Gradient Descent(429/499): loss=0.8043823140148288\n",
      "stoch Gradient Descent(430/499): loss=0.7972630307092968\n",
      "stoch Gradient Descent(431/499): loss=0.7889179832382689\n",
      "stoch Gradient Descent(432/499): loss=0.799152657306294\n",
      "stoch Gradient Descent(433/499): loss=0.7976506903083963\n",
      "stoch Gradient Descent(434/499): loss=0.7876313355745062\n",
      "stoch Gradient Descent(435/499): loss=0.7973770155832026\n",
      "stoch Gradient Descent(436/499): loss=0.7970141414125769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoch Gradient Descent(437/499): loss=0.7900002671444093\n",
      "stoch Gradient Descent(438/499): loss=0.7898304897120153\n",
      "stoch Gradient Descent(439/499): loss=0.7900841648543794\n",
      "stoch Gradient Descent(440/499): loss=0.7878673670731702\n",
      "stoch Gradient Descent(441/499): loss=0.7811744261804803\n",
      "stoch Gradient Descent(442/499): loss=0.7933050045201502\n",
      "stoch Gradient Descent(443/499): loss=0.7809315712559937\n",
      "stoch Gradient Descent(444/499): loss=0.7935161965999538\n",
      "stoch Gradient Descent(445/499): loss=0.784346073610442\n",
      "stoch Gradient Descent(446/499): loss=0.7986953618346734\n",
      "stoch Gradient Descent(447/499): loss=0.7823977919892249\n",
      "stoch Gradient Descent(448/499): loss=0.7913061202313116\n",
      "stoch Gradient Descent(449/499): loss=0.7857303057764536\n",
      "stoch Gradient Descent(450/499): loss=0.7955469524389579\n",
      "stoch Gradient Descent(451/499): loss=0.7974847871216173\n",
      "stoch Gradient Descent(452/499): loss=0.7899281461620575\n",
      "stoch Gradient Descent(453/499): loss=0.7913736624724342\n",
      "stoch Gradient Descent(454/499): loss=0.8003804914506317\n",
      "stoch Gradient Descent(455/499): loss=0.8004305735561223\n",
      "stoch Gradient Descent(456/499): loss=0.7950708703421274\n",
      "stoch Gradient Descent(457/499): loss=0.7926398884973898\n",
      "stoch Gradient Descent(458/499): loss=0.7832658630796914\n",
      "stoch Gradient Descent(459/499): loss=0.7880637738865044\n",
      "stoch Gradient Descent(460/499): loss=0.7834940104478817\n",
      "stoch Gradient Descent(461/499): loss=0.7830044095550432\n",
      "stoch Gradient Descent(462/499): loss=0.7865570234455062\n",
      "stoch Gradient Descent(463/499): loss=0.7816663949659802\n",
      "stoch Gradient Descent(464/499): loss=0.7906742929548509\n",
      "stoch Gradient Descent(465/499): loss=0.7883525956141875\n",
      "stoch Gradient Descent(466/499): loss=0.7878130968554644\n",
      "stoch Gradient Descent(467/499): loss=0.7869950890182\n",
      "stoch Gradient Descent(468/499): loss=0.7971264220877816\n",
      "stoch Gradient Descent(469/499): loss=0.7970057165319596\n",
      "stoch Gradient Descent(470/499): loss=0.7928085503920704\n",
      "stoch Gradient Descent(471/499): loss=0.786819338138512\n",
      "stoch Gradient Descent(472/499): loss=0.8026094581655878\n",
      "stoch Gradient Descent(473/499): loss=0.7697222937016269\n",
      "stoch Gradient Descent(474/499): loss=0.7881572295317095\n",
      "stoch Gradient Descent(475/499): loss=0.8003697321614721\n",
      "stoch Gradient Descent(476/499): loss=0.7904680565448534\n",
      "stoch Gradient Descent(477/499): loss=0.7971294947057087\n",
      "stoch Gradient Descent(478/499): loss=0.7808280479599018\n",
      "stoch Gradient Descent(479/499): loss=0.783154277260169\n",
      "stoch Gradient Descent(480/499): loss=0.7898388469931276\n",
      "stoch Gradient Descent(481/499): loss=0.7831162457398357\n",
      "stoch Gradient Descent(482/499): loss=0.7833605608416873\n",
      "stoch Gradient Descent(483/499): loss=0.7876664982715\n",
      "stoch Gradient Descent(484/499): loss=0.793773960713119\n",
      "stoch Gradient Descent(485/499): loss=0.7956167920131606\n",
      "stoch Gradient Descent(486/499): loss=0.786825375333901\n",
      "stoch Gradient Descent(487/499): loss=0.7876344163138183\n",
      "stoch Gradient Descent(488/499): loss=0.7970802473759392\n",
      "stoch Gradient Descent(489/499): loss=0.7957104054584849\n",
      "stoch Gradient Descent(490/499): loss=0.7882019270454577\n",
      "stoch Gradient Descent(491/499): loss=0.7980049270001424\n",
      "stoch Gradient Descent(492/499): loss=0.7880789414336581\n",
      "stoch Gradient Descent(493/499): loss=0.7870682991798136\n",
      "stoch Gradient Descent(494/499): loss=0.7816720683519499\n",
      "stoch Gradient Descent(495/499): loss=0.7898270790610809\n",
      "stoch Gradient Descent(496/499): loss=0.785138563805128\n",
      "stoch Gradient Descent(497/499): loss=0.7735349998039582\n",
      "stoch Gradient Descent(498/499): loss=0.7915646158574784\n",
      "stoch Gradient Descent(499/499): loss=0.7939806557533237\n",
      "SGD: execution time=34.845 seconds\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "from proj1_helpers import batch_iter\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.05\n",
    "batch_size = 10000\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "print(w_initial)\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(\n",
    "    y, tX, w_initial,batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Least square stochastic gradient descent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "Gradient Descent(0/49): loss=1.0, w0=53.83324755201073, w1=-13.642099301999938\n",
      "Gradient Descent(1/49): loss=1004083277570.4924, w0=-47840286.291933425, w1=22451160.509754848\n",
      "Gradient Descent(2/49): loss=8.251913580469985e+24, w0=127768620470832.92, w1=-64097460738762.164\n",
      "Gradient Descent(3/49): loss=6.784291354610181e+37, w0=-3.6532296986934156e+20, w1=1.8377016000335243e+20\n",
      "Gradient Descent(4/49): loss=5.5776973393808864e+50, w0=1.047340667076021e+27, w1=-5.269255590689982e+26\n",
      "Gradient Descent(5/49): loss=4.5856975017480705e+63, w0=-3.0030238171911826e+33, w1=1.5108608669042093e+33\n",
      "Gradient Descent(6/49): loss=3.7701259676790215e+76, w0=8.610604774822291e+39, w1=-4.332112251771585e+39\n",
      "Gradient Descent(7/49): loss=3.099604761708956e+89, w0=-2.468930447217248e+46, w1=1.242152536350795e+46\n",
      "Gradient Descent(8/49): loss=2.548336517446234e+102, w0=7.079198370712734e+52, w1=-3.5616411438332573e+52\n",
      "Gradient Descent(9/49): loss=2.09511195955497e+115, w0=-2.029828336870807e+59, w1=1.0212342901692117e+59\n",
      "Gradient Descent(10/49): loss=1.72249390652265e+128, w0=5.820154858303189e+65, w1=-2.928199201716159e+65\n",
      "Gradient Descent(11/49): loss=1.4161463994687587e+141, w0=-1.6688210505604482e+72, w1=8.396066061892703e+71\n",
      "Gradient Descent(12/49): loss=1.1642831461603844e+154, w0=4.785033674541799e+78, w1=-2.407415631913007e+78\n",
      "Gradient Descent(13/49): loss=9.572140598893294e+166, w0=-1.3720193221925407e+85, w1=6.902816130859123e+84\n",
      "Gradient Descent(14/49): loss=7.869724469270929e+179, w0=3.9340099746528856e+91, w1=-1.979254014338415e+91\n",
      "Gradient Descent(15/49): loss=6.470084991167192e+192, w0=-1.1280041199373441e+98, w1=5.675142404216304e+97\n",
      "Gradient Descent(16/49): loss=5.319372991568676e+205, w0=3.2343418110115534e+104, w1=-1.6272414290845857e+104\n",
      "Gradient Descent(17/49): loss=4.373316434337159e+218, w0=-9.273872998831274e+110, w1=4.6658118509271833e+110\n",
      "Gradient Descent(18/49): loss=3.595517115486928e+231, w0=2.6591104287630534e+117, w1=-1.3378346838489658e+117\n",
      "Gradient Descent(19/49): loss=2.9560502931497495e+244, w0=-7.624504102274755e+123, w1=3.835991888428909e+123\n",
      "Gradient Descent(20/49): loss=2.4303133749504235e+257, w0=2.1861846043244886e+130, w1=-1.0998992585360307e+130\n",
      "Gradient Descent(21/49): loss=1.9980793676447663e+270, w0=-6.268477346296484e+136, w1=3.153756353284643e+136\n",
      "Gradient Descent(22/49): loss=1.6427186718211526e+283, w0=1.7973691774841903e+143, w1=-9.042809201564242e+142\n",
      "Gradient Descent(23/49): loss=1.3505592813016118e+296, w0=-5.153621496420955e+149, w1=2.59285718666029e+149\n",
      "Gradient Descent(24/49): loss=inf, w0=1.4777050180391301e+156, w1=-7.434535265050951e+155\n",
      "Gradient Descent(25/49): loss=inf, w0=-4.23704403176375e+162, w1=2.131714576940513e+162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maxime/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in square\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/Maxime/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:70: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(26/49): loss=inf, w0=1.2148934941648552e+169, w1=-6.112294683573026e+168\n",
      "Gradient Descent(27/49): loss=inf, w0=-3.483480915230666e+175, w1=1.7525867066338204e+175\n",
      "Gradient Descent(28/49): loss=inf, w0=9.988232997426503e+181, w1=-5.025216098504703e+181\n",
      "Gradient Descent(29/49): loss=inf, w0=-2.863939858969307e+188, w1=1.4408871607370347e+188\n",
      "Gradient Descent(30/49): loss=inf, w0=8.211814359863811e+194, w1=-4.1314756804082574e+194\n",
      "Gradient Descent(31/49): loss=inf, w0=-2.3545848866090442e+201, w1=1.184623734801979e+201\n",
      "Gradient Descent(32/49): loss=inf, w0=6.75133380431142e+207, w1=-3.39668801564262e+207\n",
      "Gradient Descent(33/49): loss=inf, w0=-1.9358192773793115e+214, w1=9.739370516275073e+213\n",
      "Gradient Descent(34/49): loss=inf, w0=5.5506013823228e+220, w1=-2.7925831756244726e+220\n",
      "Gradient Descent(35/49): loss=inf, w0=-1.5915316096631008e+227, w1=8.007212354996864e+226\n",
      "Gradient Descent(36/49): loss=inf, w0=4.56342059190862e+233, w1=-2.295919070832187e+233\n",
      "Gradient Descent(37/49): loss=inf, w0=-1.3084758965650565e+240, w1=6.583120499509952e+239\n",
      "Gradient Descent(38/49): loss=inf, w0=3.751811031679816e+246, w1=-1.8875872438901347e+246\n",
      "Gradient Descent(39/49): loss=inf, w0=-1.075761965076004e+253, w1=5.4123050057521245e+252\n",
      "Gradient Descent(40/49): loss=inf, w0=3.084547157978934e+259, w1=-1.5518776983743225e+259\n",
      "Gradient Descent(41/49): loss=inf, w0=-8.844364718847137e+265, w1=4.449720383740438e+265\n",
      "Gradient Descent(42/49): loss=inf, w0=2.5359569257239533e+272, w1=-1.2758744786536135e+272\n",
      "Gradient Descent(43/49): loss=inf, w0=-7.271384360057844e+278, w1=3.658332535293562e+278\n",
      "Gradient Descent(44/49): loss=inf, w0=2.0849340923485988e+285, w1=-1.0489587465461577e+285\n",
      "Gradient Descent(45/49): loss=inf, w0=-5.978160353227226e+291, w1=3.0076939188563987e+291\n",
      "Gradient Descent(46/49): loss=inf, w0=1.714126184614329e+298, w1=-8.62400236359314e+297\n",
      "Gradient Descent(47/49): loss=inf, w0=-4.914937711890502e+304, w1=2.4727721228873775e+304\n",
      "Gradient Descent(48/49): loss=nan, w0=nan, w1=nan\n",
      "Gradient Descent(49/49): loss=nan, w0=nan, w1=nan\n",
      "SGD: execution time=4.619 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.5\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "print(w_initial)\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_GD(\n",
    "    y, tX, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plots'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b688452e5795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Time Visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradient_descent_visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntSlider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plots'"
     ]
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
