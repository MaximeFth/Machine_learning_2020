{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from costs import compute_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    return (-1/len(y))*tx.T@(y-tx@w)\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"least square gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # compute gradient computes the gradient\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        # compute loss. here MSE is used\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # TODO: update w by gradient\n",
    "        w = w-gamma*gradient\n",
    "        # ***************************************************\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return np.array(losses), np.array(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    return -1/len(y)*tx.T@e\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Least square stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        \n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        print(grad)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"stoch Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return np.array(losses), np.array(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls, loss = least_square(y, tX_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamda):\n",
    "    w = np.linalg.solve(tx.T@tx+lamda*np.eye(tx.shape[1]),tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls, loss = ridge_regression(y, tX_std,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(tx, y, w, gamma):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def loss_function(tx, y, w):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    return (error1-error2).mean()\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(tx, y, initial_w, gamma)\n",
    "        loss = loss_function(tx, y, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    return np.array(losses), np.array(ws)\n",
    "\n",
    "#################################################################################\n",
    "def decision_boundary(prob):\n",
    "    return 1 if prob >= .5 else 0\n",
    "\n",
    "def classify(predictions):\n",
    "    '''\n",
    "    input  - N element array of predictions between 0 and 1\n",
    "    output - N element array of 0s (False) and 1s (True)\n",
    "    '''\n",
    "    decision_boundary = np.vectorize(decision_boundary)\n",
    "    return decision_boundary(predictions).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maxime/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n",
      "/Users/Maxime/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: execution time=2.473 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12ad88780>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "losses, ws  = logistic_regression(y, tX_std, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, tX_std, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "plt.plot(gradient_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import batch_iter\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(30)\n",
    "print(w_initial)\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(\n",
    "    y, tX_std, w_initial,batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "plt.plot(sgd_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_std = standardize(tX_test)  # standardize par rapport a tX_train mean and std? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ...,  1.,  1., -1.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_labels(gradient_ws[-1], tX_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
