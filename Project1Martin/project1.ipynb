{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from costs import compute_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = \"train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    return (-1/len(y))*tx.T@(y-tx@w)\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"least square gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w-gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    return -1/len(y)*tx.T@e\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Least square stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # Define parameters to store w and loss\n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        \n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        print(grad)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        #print(\"stoch Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return np.array(losses), np.array(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f7f9a870b698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_square\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_std' is not defined"
     ]
    }
   ],
   "source": [
    "wls, loss = least_square(y_std, tX_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamda):\n",
    "    w = np.linalg.solve(tx.T@tx+lamda*np.eye(tx.shape[1]),tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls, loss = ridge_regression(y, tX_std,0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(tx, y, w, gamma):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def loss_function_LR(tx, y, w):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    return (error1-error2).mean()\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(tx, y, initial_w, gamma)\n",
    "        loss = loss_function_LR(tx, y, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    return np.array(ws)[-1], np.array(losses)[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_LR_update_weights(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Update weights function for  regularized logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y) + lambda_ * w\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def reg_LR_loss_function(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: logistic loss\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    # the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    # the error when label=0\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    # return average of sum of costs\n",
    "    return (error1-error2).mean()+lambda_/2*np.dot(w.T,w)/ len(tx)\n",
    "\n",
    "\n",
    "# regularized logistic regression function\n",
    "def reg_logistic_regression(y,tx, initial_w,max_iter, gamma,lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = reg_LR_update_weights(y, tx, initial_w, gamma,lambda_)\n",
    "        loss = reg_LR_loss_function(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    compute the accuracy\n",
    "    \n",
    "    :param y_pred: predictions\n",
    "    :param y: real labels\n",
    "    \n",
    "    :return acc: accuracy\n",
    "    \"\"\"\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    acc = np.count_nonzero(arr==0) / len(y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    build k indices for k-fold.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param k_fold: number of folds\n",
    "    :param seed: seed for randomization\n",
    "    \n",
    "    :return k_indices: indices \n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    :param x: matrix \n",
    "    :param degree: degree of expansion\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, logistic, regression_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes cross validation on a given data set using a given regression method, and computes the\n",
    "    weights, the train loss, the test loss, and the train and loss accuracy\n",
    "    if the degree is not none, it will perform feature expansion on the data set\n",
    "    \n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_indices: k_fold already randomly computed indices\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param logistic: boolean; if true, the loss used is the logistic one\n",
    "    :param **kwargs: differents parameters such as the regulizer lambda or the learning rate gamma\n",
    "    \"\"\"\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    print(\"FIRST\" + str(x_train.shape))\n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "\n",
    "    w_initial = np.zeros(x_train.shape[1])\n",
    "    print( \"SECONNND\" + str(x_train.shape))\n",
    "    kwargs = kwargs\n",
    "    kwargs['initial_w'] = w_initial\n",
    "\n",
    "    w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "    if logistic == True:\n",
    "        loss_test = reg_LR_loss_function(y_test, x_test, w ,kwargs['lambda_'])\n",
    "    else:\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "    y_test = (y_test*2)-1\n",
    "    y_train = (y_train*2)-1\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)\n",
    "y_std = (y+1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,y,tx,k_fold,degree,seed=0, **kwargs):\n",
    "    \"\"\"\n",
    "    regularized logistic regression function \n",
    "    \n",
    "    :param Model: model that we'll use\n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_fold: number of folds\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param seed: random seed for cross validation split\n",
    "    :param **kwargs: multiple possible parameters\n",
    "    \n",
    "    :return wf: final weights \n",
    "    \"\"\"    \n",
    "    weights = []\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    \n",
    "    logistic = False\n",
    "    if model is logistic_regression or model is reg_logistic_regression:\n",
    "        logistic = True\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in range(k_fold):\n",
    "        w, loss_train, loss_test, accuracy_train, accuracy_test = cross_validation(y, tx, k_indices, k, degree, logistic, model,max_iter=200, **kwargs)\n",
    "        weights.append(w)\n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_test)\n",
    "        accuracies_train.append(accuracy_train)\n",
    "        accuracies_test.append(accuracy_test)\n",
    "    leg = [\"train loss \"+str(i) for i in range(k_fold)]\n",
    "    plt.legend(leg)\n",
    "    for ls in losses_train:\n",
    "        plt.plot(ls)\n",
    "    \n",
    "    print(\"<-\"+\"-\"*100+\"->\")\n",
    "    for i in range(k_fold):\n",
    "        print(f'loss_train: {losses_train[i][-1]}, loss_test: {losses_test[i]}, accuracy_train: {accuracies_train[i]}, accuracy_test: {accuracies_test[i]}')\n",
    "        print(\"\\n\")\n",
    "    print(\"<-\"+\"-\"*100+\"->\")\n",
    "\n",
    "    return w, np.amin(losses_train), np.amin(losses_test), np.max(accuracies_train), np.max(accuracies_test) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function least_square at 0x000001BD78929E50>, 1e-05, 0.0001, None, 0.28669788095257115, array([ 1.48679818e-02, -1.26557052e-01, -1.27178491e-01, -1.56290748e-02,\n",
      "       -5.53207900e-01,  1.47791776e-01, -5.36682781e+00,  1.33781366e-01,\n",
      "        8.45938005e-04, -4.30181222e+01, -9.25694121e-02,  5.66453612e-02,\n",
      "        9.70762095e+00,  8.42912308e+00,  5.18781390e-04, -3.82110617e-04,\n",
      "        8.34908867e+00, -9.72974011e-04,  1.13992272e-03,  6.16855566e-02,\n",
      "       -9.89167029e-05, -3.39075152e-02, -1.03279209e-01, -6.74661451e-02,\n",
      "        3.09756120e-01, -1.66972200e-01, -1.30384467e-03, -1.30050151e+00,\n",
      "       -2.51791223e+00,  3.64046595e+01])]\n",
      "[<function least_square at 0x000001BD78929E50>, 1e-05, 0.0001, None, 0.28371222288721254, array([ 1.48679818e-02, -1.26557052e-01, -1.27178491e-01, -1.56290748e-02,\n",
      "       -5.53207900e-01,  1.47791776e-01, -5.36682781e+00,  1.33781366e-01,\n",
      "        8.45938005e-04, -4.30181222e+01, -9.25694121e-02,  5.66453612e-02,\n",
      "        9.70762095e+00,  8.42912308e+00,  5.18781390e-04, -3.82110617e-04,\n",
      "        8.34908867e+00, -9.72974011e-04,  1.13992272e-03,  6.16855566e-02,\n",
      "       -9.89167029e-05, -3.39075152e-02, -1.03279209e-01, -6.74661451e-02,\n",
      "        3.09756120e-01, -1.66972200e-01, -1.30384467e-03, -1.30050151e+00,\n",
      "       -2.51791223e+00,  3.64046595e+01])]\n",
      "[<function reg_logistic_regression at 0x000001BD7BA9EEE0>, 0.00011787686347935866, 0.01, None, 0.72767, array([ 7.14264913e-02, -6.02520894e-01, -8.07295673e-01,  1.49790359e-01,\n",
      "       -2.38820285e-01,  2.27089709e+00, -2.46648964e-01,  7.89613390e-01,\n",
      "       -2.60728181e-02, -8.26182407e-02, -4.77149200e-01,  2.21740637e-01,\n",
      "       -2.17346324e-01,  6.09438949e-01, -2.44298166e-05, -2.24438800e-03,\n",
      "        7.92199626e-01, -1.20007466e-03,  8.42817701e-03,  2.89138702e-01,\n",
      "        2.30084199e-03, -1.66902841e-01, -4.17909738e-01,  4.88023589e-02,\n",
      "        1.62578623e-01,  1.61035957e-01, -2.82748385e-01, -2.32049927e-01,\n",
      "       -2.39725399e-01, -4.15212840e-01])]\n",
      "[<function reg_logistic_regression at 0x000001BD7BA9EEE0>, 5.1794746792312125e-05, 0.1, None, 0.72874, array([ 7.20117541e-02, -6.03284777e-01, -8.05580132e-01,  1.49910109e-01,\n",
      "       -2.38882622e-01,  2.27067840e+00, -2.46970384e-01,  7.92065313e-01,\n",
      "       -2.76667121e-02, -8.27081440e-02, -4.77210906e-01,  2.23372358e-01,\n",
      "       -2.17621563e-01,  6.08673062e-01,  2.32946205e-03, -1.89754338e-03,\n",
      "        7.91751246e-01, -1.32295223e-03,  1.10733912e-02,  2.90049416e-01,\n",
      "        3.57755060e-03, -1.67755619e-01, -4.17598558e-01,  4.98910848e-02,\n",
      "        1.63161062e-01,  1.61624043e-01, -2.82647956e-01, -2.32261465e-01,\n",
      "       -2.39895115e-01, -4.15042945e-01])]\n"
     ]
    }
   ],
   "source": [
    "print(bestParametersLossTrain)\n",
    "print(bestParametersLossTest)\n",
    "print(bestParametersAccuracyTrain)\n",
    "print(bestParametersAccuracyTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK IT IS OK\n",
      "Done about : 0.0%\n",
      "Done about : 0.3333333333333333%\n",
      "Done about : 0.6666666666666666%\n",
      "Done about : 1.0%\n",
      "Done about : 1.3333333333333333%\n",
      "Done about : 1.6666666666666667%\n",
      "Done about : 2.0%\n",
      "Done about : 2.3333333333333335%\n",
      "Done about : 2.6666666666666665%\n",
      "Done about : 3.0%\n",
      "Done about : 3.3333333333333335%\n",
      "Done about : 3.6666666666666665%\n",
      "Done about : 4.0%\n",
      "Done about : 4.333333333333333%\n",
      "Done about : 4.666666666666667%\n",
      "Done about : 5.0%\n",
      "Done about : 5.333333333333333%\n",
      "Done about : 5.666666666666667%\n",
      "Done about : 6.0%\n",
      "Done about : 6.333333333333333%\n",
      "Done about : 6.666666666666667%\n",
      "Done about : 7.0%\n",
      "Done about : 7.333333333333333%\n",
      "Done about : 7.666666666666667%\n",
      "Done about : 8.0%\n",
      "Done about : 8.333333333333334%\n",
      "Done about : 8.666666666666666%\n",
      "Done about : 9.0%\n",
      "Done about : 9.333333333333334%\n",
      "Done about : 9.666666666666666%\n",
      "Done about : 10.0%\n",
      "Done about : 10.333333333333334%\n",
      "Done about : 10.666666666666666%\n",
      "Done about : 11.0%\n",
      "Done about : 11.333333333333334%\n",
      "Done about : 11.666666666666666%\n",
      "Done about : 12.0%\n",
      "Done about : 12.333333333333334%\n",
      "Done about : 12.666666666666666%\n",
      "Done about : 13.0%\n",
      "Done about : 13.333333333333334%\n",
      "Done about : 13.666666666666666%\n",
      "Done about : 14.0%\n",
      "Done about : 14.333333333333334%\n",
      "Done about : 14.666666666666666%\n",
      "Done about : 15.0%\n",
      "Done about : 15.333333333333334%\n",
      "Done about : 15.666666666666666%\n",
      "Done about : 16.0%\n",
      "Done about : 16.333333333333332%\n",
      "Done about : 16.666666666666668%\n",
      "Done about : 17.0%\n",
      "Done about : 17.333333333333332%\n",
      "Done about : 17.666666666666668%\n",
      "Done about : 18.0%\n",
      "Done about : 18.333333333333332%\n",
      "Done about : 18.666666666666668%\n",
      "Done about : 19.0%\n",
      "Done about : 19.333333333333332%\n",
      "Done about : 19.666666666666668%\n",
      "Done about : 20.0%\n",
      "Done about : 20.333333333333332%\n",
      "Done about : 20.666666666666668%\n",
      "Done about : 21.0%\n",
      "Done about : 21.333333333333332%\n",
      "Done about : 21.666666666666668%\n",
      "Done about : 22.0%\n",
      "Done about : 22.333333333333332%\n",
      "Done about : 22.666666666666668%\n",
      "Done about : 23.0%\n",
      "Done about : 23.333333333333332%\n",
      "Done about : 23.666666666666668%\n",
      "Done about : 24.0%\n",
      "Done about : 24.333333333333332%\n",
      "Done about : 24.666666666666668%\n",
      "Done about : 25.0%\n",
      "Done about : 25.333333333333332%\n",
      "Done about : 25.666666666666668%\n",
      "Done about : 26.0%\n",
      "Done about : 26.333333333333332%\n",
      "Done about : 26.666666666666668%\n",
      "Done about : 27.0%\n",
      "Done about : 27.333333333333332%\n",
      "Done about : 27.666666666666668%\n",
      "Done about : 28.0%\n",
      "Done about : 28.333333333333332%\n",
      "Done about : 28.666666666666668%\n",
      "Done about : 29.0%\n",
      "Done about : 29.333333333333332%\n",
      "Done about : 29.666666666666668%\n",
      "Done about : 30.0%\n",
      "Done about : 30.333333333333332%\n",
      "Done about : 30.666666666666668%\n",
      "Done about : 31.0%\n",
      "Done about : 31.333333333333332%\n",
      "Done about : 31.666666666666668%\n",
      "Done about : 32.0%\n",
      "Done about : 32.333333333333336%\n",
      "Done about : 32.666666666666664%\n",
      "Done about : 33.0%\n",
      "Done about : 33.333333333333336%\n",
      "Done about : 33.666666666666664%\n",
      "Done about : 34.0%\n",
      "Done about : 34.333333333333336%\n",
      "Done about : 34.666666666666664%\n",
      "Done about : 35.0%\n",
      "Done about : 35.333333333333336%\n",
      "Done about : 35.666666666666664%\n",
      "Done about : 36.0%\n",
      "Done about : 36.333333333333336%\n",
      "Done about : 36.666666666666664%\n",
      "Done about : 37.0%\n",
      "Done about : 37.333333333333336%\n",
      "Done about : 37.666666666666664%\n",
      "Done about : 38.0%\n",
      "Done about : 38.333333333333336%\n",
      "Done about : 38.666666666666664%\n",
      "Done about : 39.0%\n",
      "Done about : 39.333333333333336%\n",
      "Done about : 39.666666666666664%\n",
      "Done about : 40.0%\n",
      "Done about : 40.333333333333336%\n",
      "Done about : 40.666666666666664%\n",
      "Done about : 41.0%\n",
      "Done about : 41.333333333333336%\n",
      "Done about : 41.666666666666664%\n",
      "Done about : 42.0%\n",
      "Done about : 42.333333333333336%\n",
      "Done about : 42.666666666666664%\n",
      "Done about : 43.0%\n",
      "Done about : 43.333333333333336%\n",
      "Done about : 43.666666666666664%\n",
      "Done about : 44.0%\n",
      "Done about : 44.333333333333336%\n",
      "Done about : 44.666666666666664%\n",
      "Done about : 45.0%\n",
      "Done about : 45.333333333333336%\n",
      "Done about : 45.666666666666664%\n",
      "Done about : 46.0%\n",
      "Done about : 46.333333333333336%\n",
      "Done about : 46.666666666666664%\n",
      "Done about : 47.0%\n",
      "Done about : 47.333333333333336%\n",
      "Done about : 47.666666666666664%\n",
      "Done about : 48.0%\n",
      "Done about : 48.333333333333336%\n",
      "Done about : 48.666666666666664%\n",
      "Done about : 49.0%\n",
      "Done about : 49.333333333333336%\n",
      "Done about : 49.666666666666664%\n",
      "Done about : 50.0%\n",
      "Done about : 50.333333333333336%\n",
      "Done about : 50.666666666666664%\n",
      "Done about : 51.0%\n",
      "Done about : 51.333333333333336%\n",
      "Done about : 51.666666666666664%\n",
      "Done about : 52.0%\n",
      "Done about : 52.333333333333336%\n",
      "Done about : 52.666666666666664%\n",
      "Done about : 53.0%\n",
      "Done about : 53.333333333333336%\n",
      "Done about : 53.666666666666664%\n",
      "Done about : 54.0%\n",
      "Done about : 54.333333333333336%\n",
      "Done about : 54.666666666666664%\n",
      "Done about : 55.0%\n",
      "Done about : 55.333333333333336%\n",
      "Done about : 55.666666666666664%\n",
      "Done about : 56.0%\n",
      "Done about : 56.333333333333336%\n",
      "Done about : 56.666666666666664%\n",
      "Done about : 57.0%\n",
      "Done about : 57.333333333333336%\n",
      "Done about : 57.666666666666664%\n",
      "Done about : 58.0%\n",
      "Done about : 58.333333333333336%\n",
      "Done about : 58.666666666666664%\n",
      "Done about : 59.0%\n",
      "Done about : 59.333333333333336%\n",
      "Done about : 59.666666666666664%\n",
      "Done about : 60.0%\n",
      "Done about : 60.333333333333336%\n",
      "Done about : 60.666666666666664%\n",
      "Done about : 61.0%\n",
      "Done about : 61.333333333333336%\n",
      "Done about : 61.666666666666664%\n",
      "Done about : 62.0%\n",
      "Done about : 62.333333333333336%\n",
      "Done about : 62.666666666666664%\n",
      "Done about : 63.0%\n",
      "Done about : 63.333333333333336%\n",
      "Done about : 63.666666666666664%\n",
      "Done about : 64.0%\n",
      "Done about : 64.33333333333333%\n",
      "Done about : 64.66666666666667%\n",
      "Done about : 65.0%\n",
      "Done about : 65.33333333333333%\n",
      "Done about : 65.66666666666667%\n",
      "Done about : 66.0%\n",
      "Done about : 66.33333333333333%\n",
      "Done about : 66.66666666666667%\n",
      "Done about : 67.0%\n",
      "Done about : 67.33333333333333%\n",
      "Done about : 67.66666666666667%\n",
      "Done about : 68.0%\n",
      "Done about : 68.33333333333333%\n",
      "Done about : 68.66666666666667%\n",
      "Done about : 69.0%\n",
      "Done about : 69.33333333333333%\n",
      "Done about : 69.66666666666667%\n",
      "Done about : 70.0%\n",
      "Done about : 70.33333333333333%\n",
      "Done about : 70.66666666666667%\n",
      "Done about : 71.0%\n",
      "Done about : 71.33333333333333%\n",
      "Done about : 71.66666666666667%\n",
      "Done about : 72.0%\n",
      "Done about : 72.33333333333333%\n",
      "Done about : 72.66666666666667%\n",
      "Done about : 73.0%\n",
      "Done about : 73.33333333333333%\n",
      "Done about : 73.66666666666667%\n",
      "Done about : 74.0%\n",
      "Done about : 74.33333333333333%\n",
      "Done about : 74.66666666666667%\n",
      "Done about : 75.0%\n",
      "Done about : 75.33333333333333%\n",
      "Done about : 75.66666666666667%\n",
      "Done about : 76.0%\n",
      "Done about : 76.33333333333333%\n",
      "Done about : 76.66666666666667%\n",
      "Done about : 77.0%\n",
      "Done about : 77.33333333333333%\n",
      "Done about : 77.66666666666667%\n",
      "Done about : 78.0%\n",
      "Done about : 78.33333333333333%\n",
      "Done about : 78.66666666666667%\n",
      "Done about : 79.0%\n",
      "Done about : 79.33333333333333%\n",
      "Done about : 79.66666666666667%\n",
      "Done about : 80.0%\n",
      "Done about : 80.33333333333333%\n",
      "Done about : 80.66666666666667%\n",
      "Done about : 81.0%\n",
      "Done about : 81.33333333333333%\n",
      "Done about : 81.66666666666667%\n",
      "Done about : 82.0%\n",
      "Done about : 82.33333333333333%\n",
      "Done about : 82.66666666666667%\n",
      "Done about : 83.0%\n",
      "Done about : 83.33333333333333%\n",
      "Done about : 83.66666666666667%\n",
      "Done about : 84.0%\n",
      "Done about : 84.33333333333333%\n",
      "Done about : 84.66666666666667%\n",
      "Done about : 85.0%\n",
      "Done about : 85.33333333333333%\n",
      "Done about : 85.66666666666667%\n",
      "Done about : 86.0%\n",
      "Done about : 86.33333333333333%\n",
      "Done about : 86.66666666666667%\n",
      "Done about : 87.0%\n",
      "Done about : 87.33333333333333%\n",
      "Done about : 87.66666666666667%\n",
      "Done about : 88.0%\n",
      "Done about : 88.33333333333333%\n",
      "Done about : 88.66666666666667%\n",
      "Done about : 89.0%\n",
      "Done about : 89.33333333333333%\n",
      "Done about : 89.66666666666667%\n",
      "Done about : 90.0%\n",
      "Done about : 90.33333333333333%\n",
      "Done about : 90.66666666666667%\n",
      "Done about : 91.0%\n",
      "Done about : 91.33333333333333%\n",
      "Done about : 91.66666666666667%\n",
      "Done about : 92.0%\n",
      "Done about : 92.33333333333333%\n",
      "Done about : 92.66666666666667%\n",
      "Done about : 93.0%\n",
      "Done about : 93.33333333333333%\n",
      "Done about : 93.66666666666667%\n",
      "Done about : 94.0%\n",
      "Done about : 94.33333333333333%\n",
      "Done about : 94.66666666666667%\n",
      "Done about : 95.0%\n",
      "Done about : 95.33333333333333%\n",
      "Done about : 95.66666666666667%\n",
      "Done about : 96.0%\n",
      "Done about : 96.33333333333333%\n",
      "Done about : 96.66666666666667%\n",
      "Done about : 97.0%\n",
      "Done about : 97.33333333333333%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done about : 97.66666666666667%\n",
      "Done about : 98.0%\n",
      "Done about : 98.33333333333333%\n",
      "Done about : 98.66666666666667%\n",
      "Done about : 99.0%\n",
      "Done about : 99.33333333333333%\n",
      "Done about : 99.66666666666667%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wf = train(reg_logistic_regression,y_std,tX_std,5,0,5,gamma=0.1757, lambda_ = 0)\n",
    "#w, lossTr, lossTe, accTr, accTe = train(names[0],y_std,tX_std,5,degrees[0],None,gamma = gammas[3], lambda_ = 0)\n",
    "#\n",
    "#\n",
    "names = [least_squares_GD,least_square, ridge_regression, logistic_regression,reg_logistic_regression]\n",
    "initial_w  = np.zeros(tX_std.shape[1])\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "gammas = np.array([0.0001,0.001,0.01,0.1])\n",
    "degrees = np.array([0,1,2,3,4,5,6,7])\n",
    "#parameters = [{'gamma' : gammas[0],'lambda_' : lambdas[0]}]\n",
    "#wf = train(reg_logistic_regression,y_std,tX_std,5,0,None,**(parameters[0]))\n",
    "bestParametersLossTrain = [names[0], lambdas[0], gammas[0],degrees[0], 1, initial_w ]\n",
    "bestParametersLossTest = [names[0], lambdas[0], gammas[0],degrees[0], 1, initial_w ]\n",
    "bestParametersAccuracyTrain = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "bestParametersAccuracyTest = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "#parameter = [{'initial_w':initial_w,'max_iter':200,'gamma' : gamma}]\n",
    "#ToDelete,_,_,_,_ = train(logistic_regression,y_std,tX_std,5,None,0,**parameter[0])\n",
    "print(\"OK IT IS OK\")\n",
    "for indexML, model_name in enumerate(names):\n",
    "    for indexL,lambda_ in enumerate(lambdas):\n",
    "        for indexG,gamma in enumerate(gammas):\n",
    "            print(\"Done about : \" + str(100*(indexML*len(lambdas)*len(gammas)+ indexL*len(gammas) + indexG)/(len(names)*len(lambdas)*len(gammas))) + \"%\")\n",
    "            #for degree in degrees:1\n",
    "            parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "            wTemp, lossTr, lossTe, accTr, accTe = train(names[indexML],y_std,tX_std,5,None,0,**parameters[indexML])\n",
    "            if lossTr < bestParametersLossTrain[4]:\n",
    "                bestParametersLossTrain = [model_name,lambda_, gamma, None, lossTr, wTemp] \n",
    "            if lossTe < bestParametersLossTest[4]:\n",
    "                bestParametersLossTest = [model_name,lambda_, gamma, None, lossTe, wTemp]\n",
    "            if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "            if accTe > bestParametersAccuracyTest[4]:\n",
    "                bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp]                    \n",
    "                           \n",
    "                \n",
    "\n",
    "#The parameters are : Lambda, degrees, gamma\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# standardize wrt the tx train mean and std\n",
    "def standardize_test(tx, tx_test):\n",
    "    centered_data = tx_test - np.mean(tx, axis=0)\n",
    "    std_data = centered_data / np.std(tx, axis=0)\n",
    "    return std_data\n",
    "tX_test_std = standardize_test(tX, tX_test)\n",
    "OUTPUT_PATH = 'outLogRegD23v2.csv' \n",
    "y_pred = predict_labels(wk, tX_test_std)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsdfqfdsf = np.array([1,2,3])\n",
    "np.max(xsdfqfdsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
