{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from costs import compute_loss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param w: weights\n",
    "    :return grad: gradient\n",
    "    \"\"\"\n",
    "    grad = (-1/len(y))*tx.T@(y-tx@w)\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Least square gradient descent\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: max number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :return ws: weights\n",
    "    :return ls: loss\n",
    "    \"\"\"    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w-gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param w: weights\n",
    "    :return grad: gradient\n",
    "    \"\"\"    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    grad = -1/len(y)*tx.T@e\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Least square stochastic gradient descent\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param initial_w: initial weights\n",
    "    :param batch_size: 1 if sgd\n",
    "    :param max_iter: max number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :return ws: weights\n",
    "    :return ls: loss\n",
    "    \"\"\"   \n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    \"\"\"\n",
    "    Solves the closed form least square equation to obtain optimal weights\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :returns w,l: weights and loss of the model\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    l = compute_loss(y, tx, w)\n",
    "    return w, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Solves the closed form of Ridge regression equation to obtain optimal weights\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param lambda_: regulizer\n",
    "    :returns w,l: weights and loss of the model\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T@tx+lambda_*np.eye(tx.shape[1]),tx.T@y)\n",
    "    l = compute_loss(y, tx, w)\n",
    "    return w, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \n",
    "    :param z: \n",
    "    :return z:\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Update weights function for logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def LR_loss_function(y, tx, w):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    \n",
    "    :return loss: logistic loss\n",
    "    \"\"\" \n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    loss = (error1-error2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(y, tx, initial_w, gamma)\n",
    "        loss = LR_loss_function(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_LR_update_weights(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Update weights function for  regularized logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y) + lambda_ * w\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def reg_LR_loss_function(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: logistic loss\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    # the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    # the error when label=0\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    # return average of sum of costs\n",
    "    return (error1-error2).mean()+lambda_/2*np.dot(w.T,w)/ len(tx)\n",
    "\n",
    "\n",
    "# regularized logistic regression function\n",
    "def reg_logistic_regression(y,tx, initial_w,max_iter, gamma,lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        if(iter_n > 800):\n",
    "            gamma = gamma-gamma/30\n",
    "        w = reg_LR_update_weights(y, tx, initial_w, gamma,lambda_)\n",
    "        loss = reg_LR_loss_function(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    compute the accuracy\n",
    "    \n",
    "    :param y_pred: predictions\n",
    "    :param y: real labels\n",
    "    \n",
    "    :return acc: accuracy\n",
    "    \"\"\"\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    acc = np.count_nonzero(arr==0) / len(y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    build k indices for k-fold.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param k_fold: number of folds\n",
    "    :param seed: seed for randomization\n",
    "    \n",
    "    :return k_indices: indices \n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    :param x: matrix \n",
    "    :param degree: degree of expansion\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, regression_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes cross validation on a given data set using a given regression method, and computes the\n",
    "    weights, the train loss, the test loss, and the train and loss accuracy\n",
    "    if the degree is not none, it will perform feature expansion on the data set\n",
    "    \n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_indices: k_fold already randomly computed indices\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param logistic: boolean; if true, the loss used is the logistic one\n",
    "    :param **kwargs: differents parameters such as the regulizer lambda or the learning rate gamma\n",
    "    \"\"\"\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "    \n",
    "\n",
    "    w_initial = np.zeros(x_train.shape[1])\n",
    "    kwargs = kwargs\n",
    "    kwargs['initial_w'] = w_initial\n",
    "\n",
    "    \n",
    "    if regression_method is reg_logistic_regression:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "        loss_test = reg_LR_loss_function(y_test, x_test, w ,kwargs['lambda_'])\n",
    "        \n",
    "    elif regression_method is logistic_regression:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "        loss_test = LR_loss_function(y_test, x_test, w)\n",
    "        \n",
    "    elif regression_method is least_square:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "    elif regression_method is ridge_regression:\n",
    "        w, loss_train = regression_method(y_train,x_train, kwargs['lambda_'])\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    else: \n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "\n",
    "        \n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "    y_test = (y_test*2)-1\n",
    "    y_train = (y_train*2)-1\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n",
    "\n",
    "def evaluate(tx, wf, degree):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to evaluate weights over all the train model\n",
    "    \n",
    "    :param tx: train features\n",
    "    :param wf: wights to evaluate\n",
    "    :param degree: degree of expansion\n",
    "    :return acc: accuracy of the weights over the train model\n",
    "    \"\"\"\n",
    "    if degree is not None:\n",
    "        tx =build_poly(tx, degree)\n",
    "    if isinstance(wf, list):\n",
    "        wk =np.mean(wf, axis =0)\n",
    "\n",
    "    else:\n",
    "        wk = wf\n",
    "                \n",
    "    y_pred = predict_labels(wk, tx)\n",
    "    acc = compute_accuracy(y_std*2-1, y_pred)\n",
    "    return acc\n",
    "def remove_outliers_IQR(tx, y, high,low):\n",
    "    \"\"\"\n",
    "    removes outliers using IQR\n",
    "    \n",
    "    :param tx: features\n",
    "    :param y: labels\n",
    "    :param high: high IQR\n",
    "    :param low: low IQR\n",
    "    :returns tX, Y: features and labels without outliers\n",
    "    \"\"\"\n",
    "    Q1 = np.quantile(tX,low,axis = 0)\n",
    "    Q3 = np.quantile(tX,high, axis = 0)\n",
    "    IQR = Q3 - Q1\n",
    "    tX_no_outliers = tX[~((tX < (Q1 - 1.5 * IQR)) |(tX > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    y_no_outliers = y_std[~((tX < (Q1 - 1.5 * IQR)) |(tX > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return tX_no_outliers, y_no_outliers\n",
    "\n",
    "def replace(tX, value):\n",
    "    \"\"\"\n",
    "    Replaces invalid values with the mean of all the values in the cooresponding feature \n",
    "\n",
    "    :param tX: features\n",
    "    :param value: value to replace\n",
    "    :return tX: tX with replaced values\n",
    "    \"\"\"\n",
    "    for i in range(tX.shape[1]):\n",
    "        data = tX[:, i].copy()\n",
    "        np.delete(data, np.where(data == value))  \n",
    "        data_median = np.median(data)  \n",
    "        tX[tX[:, i] == value,i] = data_median  \n",
    "    return tX\n",
    "\n",
    "def standardize_test(tx, tx_test):\n",
    "    \"\"\"\n",
    "    Standardize wrt the tx train mean and std\n",
    "    \n",
    "    :param tx: train features\n",
    "    :param tx_test: test features\n",
    "    :return: std_data: standardized version of tx_test\n",
    "    \"\"\"\n",
    "    value = -999\n",
    "    tx_test = replace(tx_test, value)\n",
    "    centered_data = tx_test - np.mean(tx, axis=0)\n",
    "    std_data = centered_data / np.std(tx, axis=0)\n",
    "    return std_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,y,tx,k_fold,degree,seed=0, **kwargs):\n",
    "    \"\"\"\n",
    "    regularized logistic regression function \n",
    "    \n",
    "    :param Model: model that we'll use\n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_fold: number of folds\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param seed: random seed for cross validation split\n",
    "    :param **kwargs: multiple possible parameters\n",
    "    \n",
    "    :return wf: final weights \n",
    "    \"\"\"    \n",
    "    weights = []\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    \n",
    "    logistic = False\n",
    "    if model is logistic_regression or model is reg_logistic_regression:\n",
    "        logistic = True\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in tqdm(range(k_fold)):\n",
    "        w, loss_train, loss_test, accuracy_train, accuracy_test = cross_validation(y, tx, k_indices, k, degree, model, **kwargs)\n",
    "        weights.append(w)\n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_test)\n",
    "        accuracies_train.append(accuracy_train)\n",
    "        accuracies_test.append(accuracy_test)\n",
    "    leg = [\"train loss \"+str(i) for i in range(k_fold)]\n",
    "    plt.legend(leg)\n",
    "    plt.plot(losses_train[-1])    \n",
    "    print(\"<-\"+\"-\"*75+\"->\")\n",
    "    if degree is not None:\n",
    "        degree = int(degree)\n",
    "    \n",
    "    print(\"{:15.14}|{:15.14}|{:15.14}|{:15.14}|{:15.14}\".format(\"Train losses\",\"Test losses\",\"Train accuracy\",\"Test Accuracy\", \"Evaluation\"))\n",
    "    for i in range(k_fold):\n",
    "        if model is least_square or model is ridge_regression:\n",
    "            print(\"{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}\".format(losses_train[i], losses_test[i] ,accuracies_train[i], accuracies_test[i], evaluate(tX_std, np.array(weights[i]), degree)))\n",
    "        else:\n",
    "            print(\"{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}\".format(losses_train[i][-1], losses_test[i] ,accuracies_train[i], accuracies_test[i], evaluate(tX_std, np.array(weights[i]), degree)))\n",
    "        print(\"{:15.1}|{:15.14}|{:15.14}|{:15.14}|{:15.14}\".format(\"\",\"\",\"\",\"\",\"\"))\n",
    "    print(\"<-\"+\"-\"*75+\"->\")\n",
    "    print(f\"evaluation mean w: {evaluate(tX_std, weights, degree)}\")\n",
    "\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y_std = (y+1)/2\n",
    "value = -999\n",
    "tX = replace(tX, value)\n",
    "tX = standardize(tX_std)\n",
    "\n",
    "tX_no_outliers_std, y_no_outliers = remove_outliers_IQR(tX,y_std, 0.85, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls = train(least_square,y_std,tX_std,5,None,seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least squares Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lsGD = train(least_squares_GD,y_no_outliers,tX_no_outliers_std,5,None,gamma=0.0277184057,seed=0, max_iter = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least squares Stochastic Gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lsSGD = train(least_squares_SGD,y_no_outliers,tX_no_outliers_std,5,None,gamma=0.0257184057,seed=0,batch_size = 10000, max_iter = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_LR = train(logistic_regression,y_no_outliers,tX_no_outliers_std,5,2,gamma=0.2307184057,seed=0, max_iter = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test regularized Logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_regLR = train(reg_logistic_regression,y_no_outliers,tX_no_outliers_std,5,2,gamma=0.227184057,seed=0, max_iter = 300, lambda_ = 0.000770)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test_std = standardize_test(tX, tX_test)\n",
    "tX_test_std =build_poly(tX_test_std, 2)\n",
    "OUTPUT_PATH = 'outLogRegD23v2.csv' \n",
    "y_pred = predict_labels(np.mean(w_LR,axis=0), tX_test_std)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
