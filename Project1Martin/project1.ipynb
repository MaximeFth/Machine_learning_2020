{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from costs import compute_loss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = \"train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param w: weights\n",
    "    :return grad: gradient\n",
    "    \"\"\"\n",
    "    grad = (-1/len(y))*tx.T@(y-tx@w)\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Least square gradient descent\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: max number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :return ws: weights\n",
    "    :return ls: loss\n",
    "    \"\"\"    \n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w-gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the gradient.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param w: weights\n",
    "    :return grad: gradient\n",
    "    \"\"\"    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    grad = -1/len(y)*tx.T@e\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Least square stochastic gradient descent\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param initial_w: initial weights\n",
    "    :param batch_size: 1 if sgd\n",
    "    :param max_iter: max number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :return ws: weights\n",
    "    :return ls: loss\n",
    "    \"\"\"   \n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    \"\"\"\n",
    "    Solves the closed form least square equation to obtain optimal weights\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :returns w,l: weights and loss of the model\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    l = compute_loss(y, tx, w)\n",
    "    return w, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Solves the closed form of Ridge regression equation to obtain optimal weights\n",
    "    \n",
    "    :param y: labels\n",
    "    :param tx: features\n",
    "    :param lambda_: regulizer\n",
    "    :returns w,l: weights and loss of the model\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T@tx+lambda_*np.eye(tx.shape[1]),tx.T@y)\n",
    "    l = compute_loss(y, tx, w)\n",
    "    return w, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \n",
    "    :param z: \n",
    "    :return z:\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Update weights function for logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def LR_loss_function(y, tx, w):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    \n",
    "    :return loss: logistic loss\n",
    "    \"\"\" \n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    loss = (error1-error2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    \"\"\"\n",
    "    Logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(y, tx, initial_w, gamma)\n",
    "        loss = LR_loss_function(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_LR_update_weights(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Update weights function for  regularized logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y) + lambda_ * w\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def reg_LR_loss_function(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: logistic loss\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    # the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    # the error when label=0\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    # return average of sum of costs\n",
    "    return (error1-error2).mean()+lambda_/2*np.dot(w.T,w)/ len(tx)\n",
    "\n",
    "\n",
    "# regularized logistic regression function\n",
    "def reg_logistic_regression(y,tx, initial_w,max_iter, gamma,lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        if(iter_n > 800):\n",
    "            gamma = gamma-gamma/30\n",
    "        w = reg_LR_update_weights(y, tx, initial_w, gamma,lambda_)\n",
    "        loss = reg_LR_loss_function(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    compute the accuracy\n",
    "    \n",
    "    :param y_pred: predictions\n",
    "    :param y: real labels\n",
    "    \n",
    "    :return acc: accuracy\n",
    "    \"\"\"\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    acc = np.count_nonzero(arr==0) / len(y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    build k indices for k-fold.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param k_fold: number of folds\n",
    "    :param seed: seed for randomization\n",
    "    \n",
    "    :return k_indices: indices \n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    :param x: matrix \n",
    "    :param degree: degree of expansion\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, regression_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes cross validation on a given data set using a given regression method, and computes the\n",
    "    weights, the train loss, the test loss, and the train and loss accuracy\n",
    "    if the degree is not none, it will perform feature expansion on the data set\n",
    "    \n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_indices: k_fold already randomly computed indices\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param logistic: boolean; if true, the loss used is the logistic one\n",
    "    :param **kwargs: differents parameters such as the regulizer lambda or the learning rate gamma\n",
    "    \"\"\"\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "    \n",
    "\n",
    "    w_initial = np.zeros(x_train.shape[1])\n",
    "    kwargs = kwargs\n",
    "    kwargs['initial_w'] = w_initial\n",
    "\n",
    "    \n",
    "    if regression_method is reg_logistic_regression:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "        loss_test = reg_LR_loss_function(y_test, x_test, w ,kwargs['lambda_'])\n",
    "        \n",
    "    elif regression_method is logistic_regression:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "        loss_test = LR_loss_function(y_test, x_test, w)\n",
    "        \n",
    "    elif regression_method is least_square:\n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "        \n",
    "    elif regression_method is ridge_regression:\n",
    "        w, loss_train = regression_method(y_train,x_train, kwargs['lambda_'])\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    else: \n",
    "        w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "\n",
    "        \n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "    y_test = (y_test*2)-1\n",
    "    y_train = (y_train*2)-1\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n",
    "\n",
    "def evaluate(tx, wf, degree):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to evaluate weights over all the train model\n",
    "    \n",
    "    :param tx: train features\n",
    "    :param wf: wights to evaluate\n",
    "    :param degree: degree of expansion\n",
    "    :return acc: accuracy of the weights over the train model\n",
    "    \"\"\"\n",
    "    if degree is not None:\n",
    "        tx =build_poly(tx, degree)\n",
    "    if isinstance(wf, list):\n",
    "        wk =np.mean(wf, axis =0)\n",
    "\n",
    "    else:\n",
    "        wk = wf\n",
    "                \n",
    "    y_pred = predict_labels(wk, tx)\n",
    "    acc = compute_accuracy(y_std*2-1, y_pred)\n",
    "    return acc\n",
    "def remove_outliers_IQR(tx, y, high,low):\n",
    "    \"\"\"\n",
    "    removes outliers using IQR\n",
    "    \n",
    "    :param tx: features\n",
    "    :param y: labels\n",
    "    :param high: high IQR\n",
    "    :param low: low IQR\n",
    "    :returns tX, Y: features and labels without outliers\n",
    "    \"\"\"\n",
    "    Q1 = np.quantile(tX,low,axis = 0)\n",
    "    Q3 = np.quantile(tX,high, axis = 0)\n",
    "    IQR = Q3 - Q1\n",
    "    tX_no_outliers = tX[~((tX < (Q1 - 1.5 * IQR)) |(tX > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    y_no_outliers = y_std[~((tX < (Q1 - 1.5 * IQR)) |(tX > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return tX_no_outliers, y_no_outliers\n",
    "\n",
    "def replace(tX, value):\n",
    "    \"\"\"\n",
    "    Replaces invalid values with the mean of all the values in the cooresponding feature \n",
    "\n",
    "    :param tX: features\n",
    "    :param value: value to replace\n",
    "    :return tX: tX with replaced values\n",
    "    \"\"\"\n",
    "    for i in range(tX.shape[1]):\n",
    "        data = tX[:, i].copy()\n",
    "        np.delete(data, np.where(data == value))  \n",
    "        data_median = np.median(data)  \n",
    "        tX[tX[:, i] == value,i] = data_median  \n",
    "    return tX\n",
    "\n",
    "def standardize_test(tx, tx_test):\n",
    "    \"\"\"\n",
    "    Standardize wrt the tx train mean and std\n",
    "    \n",
    "    :param tx: train features\n",
    "    :param tx_test: test features\n",
    "    :return: std_data: standardized version of tx_test\n",
    "    \"\"\"\n",
    "    value = -999\n",
    "    tx_test = replace(tx_test, value)\n",
    "    centered_data = tx_test - np.mean(tx, axis=0)\n",
    "    std_data = centered_data / np.std(tx, axis=0)\n",
    "    return std_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,y,tx,k_fold,degree,seed=0, **kwargs):\n",
    "    \"\"\"\n",
    "    regularized logistic regression function \n",
    "    \n",
    "    :param Model: model that we'll use\n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_fold: number of folds\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param seed: random seed for cross validation split\n",
    "    :param **kwargs: multiple possible parameters\n",
    "    \n",
    "    :return wf: final weights \n",
    "    \"\"\"    \n",
    "    weights = []\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    \n",
    "    logistic = False\n",
    "    if model is logistic_regression or model is reg_logistic_regression:\n",
    "        logistic = True\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in tqdm(range(k_fold)):\n",
    "        w, loss_train, loss_test, accuracy_train, accuracy_test = cross_validation(y, tx, k_indices, k, degree, model, **kwargs)\n",
    "        weights.append(w)\n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_test)\n",
    "        accuracies_train.append(accuracy_train)\n",
    "        accuracies_test.append(accuracy_test)\n",
    "    leg = [\"train loss \"+str(i) for i in range(k_fold)]\n",
    "    plt.legend(leg)\n",
    "    plt.plot(losses_train[-1])    \n",
    "    print(\"<-\"+\"-\"*75+\"->\")\n",
    "    if degree is not None:\n",
    "        degree = int(degree)\n",
    "    \n",
    "    print(\"{:15.14}|{:15.14}|{:15.14}|{:15.14}|{:15.14}\".format(\"Train losses\",\"Test losses\",\"Train accuracy\",\"Test Accuracy\", \"Evaluation\"))\n",
    "    for i in range(k_fold):\n",
    "        if model is least_square or model is ridge_regression:\n",
    "            print(\"{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}\".format(losses_train[i], losses_test[i] ,accuracies_train[i], accuracies_test[i], evaluate(tX_std, np.array(weights[i]), degree)))\n",
    "        else:\n",
    "            print(\"{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}|{:< 15f}\".format(losses_train[i][-1], losses_test[i] ,accuracies_train[i], accuracies_test[i], evaluate(tX_std, np.array(weights[i]), degree)))\n",
    "        print(\"{:15.1}|{:15.14}|{:15.14}|{:15.14}|{:15.14}\".format(\"\",\"\",\"\",\"\",\"\"))\n",
    "    print(\"<-\"+\"-\"*75+\"->\")\n",
    "    print(f\"evaluation mean w: {evaluate(tX_std, weights, degree)}\")\n",
    "\n",
    "    return weights, sum(accuracies_train)/len(accuracies_train),sum(accuracies_test)/len(accuracies_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y_std = (y+1)/2\n",
    "\n",
    "\n",
    "tX_no_outliers, y_no_outliers = remove_outliers_IQR(tX,y_std, 0.85, 0)\n",
    "\n",
    "tX_no_outliers_std = standardize(tX_no_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls = train(least_square,y_std,tX_std,5,None,seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least squares Gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lsGD = train(least_squares_GD,y_no_outliers,tX_no_outliers_std,5,None,gamma=0.0277184057,seed=0, max_iter = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test least squares Stochastic Gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lsSGD = train(least_squares_SGD,y_no_outliers,tX_no_outliers_std,5,None,gamma=0.0257184057,seed=0,batch_size = 10000, max_iter = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_LR = train(logistic_regression,y_no_outliers,tX_no_outliers_std,5,2,gamma=0.2307184057,seed=0, max_iter = 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test regularized Logistic regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_regLR = train(reg_logistic_regression,y_no_outliers,tX_no_outliers_std,5,2,gamma=0.227184057,seed=0, max_iter = 300, lambda_ = 0.000770)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done about :0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:39<00:00,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<----------------------------------------------------------------------------->\n",
      "Train losses   |Test losses    |Train accuracy |Test Accuracy  |Evaluation     \n",
      " 0.337345      | 0.337274      | 0.638500      | 0.635900      | 0.637980      \n",
      "               |               |               |               |               \n",
      " 0.337080      | 0.338328      | 0.637770      | 0.637780      | 0.637772      \n",
      "               |               |               |               |               \n",
      " 0.337111      | 0.338429      | 0.637640      | 0.637760      | 0.637664      \n",
      "               |               |               |               |               \n",
      " 0.336867      | 0.339321      | 0.637555      | 0.639660      | 0.637976      \n",
      "               |               |               |               |               \n",
      " 0.338314      | 0.333264      | 0.638070      | 0.638300      | 0.638116      \n",
      "               |               |               |               |               \n",
      "<----------------------------------------------------------------------------->\n",
      "evaluation mean w: 0.637888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-d67e8969402f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m                             \u001b[0mbestParametersAccuracyTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccTe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwTemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                     \u001b[0mwTemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccTr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccTe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexML\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_std\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_std\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexML\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0maccTr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbestParametersAccuracyTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                         \u001b[0mbestParametersAccuracyTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccTr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwTemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "names = [least_squares_GD,least_square, ridge_regression, logistic_regression,reg_logistic_regression]\n",
    "initial_w  = np.zeros(tX_std.shape[1])\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "gammas = np.array([0.0001,0.001,0.01,0.1])\n",
    "degrees = np.array([0,1,2,3,4,5,6,7])\n",
    "lambda_ = lambdas[0]\n",
    "gamma = gammas[0]\n",
    "degree = degrees[0]\n",
    "#bestParametersLossTrain = [names[0], lambdas[0], gammas[0],degrees[0], 1, initial_w ]\n",
    "#bestParametersLossTest = [names[0], lambdas[0], gammas[0],degrees[0], 1, initial_w ]\n",
    "bestParametersAccuracyTrain = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "bestParametersAccuracyTest = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "for indexML, model_name in enumerate(names):\n",
    "        print(\"Done about :\" + str(indexML/len(names)))\n",
    "        if(model_name is least_square or model_name is ridge_regression): #Skip the lambdas\n",
    "            if(model_name is ridge_regression):\n",
    "                for indexL,lambda_ in enumerate(lambdas):\n",
    "                    parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                    wTemp, accTr, accTe = train(names[indexML],y_std,tX_std,5,None,0,**parameters[indexML])                \n",
    "                    if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                        bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                    if accTe > bestParametersAccuracyTest[4]:\n",
    "                        bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp] \n",
    "            else:\n",
    "                parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                wTemp, accTr, accTe = train(names[indexML],y_std,tX_std,5,None,0,**parameters[indexML])                \n",
    "                if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                    bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                if accTe > bestParametersAccuracyTest[4]:\n",
    "                    bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp] \n",
    "        else:    \n",
    "            for indexG,gamma in enumerate(gammas):\n",
    "                if(model_name is reg_logistic_regression):\n",
    "                    for indexL,lambda_ in enumerate(lambdas):\n",
    "                        parameters = [{'initial_w': initial_w, 'max_iter':200,'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma}, {'initial_w':initial_w,'max_iter':200,'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                        wTemp, accTr, accTe = train(names[indexML],y_std,tX_std,5,None,0,**parameters[indexML])\n",
    "                        if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                            bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                        if accTe > bestParametersAccuracyTest[4]:\n",
    "                            bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp]\n",
    "                else:\n",
    "                    wTemp, accTr, accTe = train(names[indexML],y_std,tX_std,5,None,0,**parameters[indexML])\n",
    "                    if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                        bestParametersAccuracyTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                    if accTe > bestParametersAccuracyTest[4]:\n",
    "                        bestParametersAccuracyTest = [model_name,lambda_, gamma, None, accTe, wTemp]                        \n",
    "                #print(\"Done about : \" + str(100*(indexML*len(lambdas)*len(gammas)+ indexL*len(gammas) + indexG)/(len(names)*len(lambdas)*len(gammas))) + \"%\")\n",
    "             #   if lossTr < bestParametersLossTrain[4]:\n",
    "             #       bestParametersLossTrain = [model_name,lambda_, gamma, None, lossTr, wTemp] \n",
    "             #   if lossTe < bestParametersLossTest[4]:\n",
    "             #       bestParametersLossTest = [model_name,lambda_, gamma, None, lossTe, wTemp]                  \n",
    "                           \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "tX_test_std = standardize_test(tX, tX_test)\n",
    "tX_test_std =build_poly(tX_test_std, 2)\n",
    "OUTPUT_PATH = 'outLogRegD23v2.csv' \n",
    "y_pred = predict_labels(np.mean(w_LR,axis=0), tX_test_std)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
